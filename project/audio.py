import os
import toml
import torch
import datetime
import subprocess
import streamlit as st
import streamlit_antd_components as sac
from openai import OpenAI
from utils.utils import (get_whisper_result, generate_srt_from_result, parse_srt_file, convert_to_srt, openai_whisper)


def audio():
    project_dir = os.path.dirname(os.path.abspath(__file__)).replace("\\", "/")
    cache_dir = project_dir + "/cache/"  # æœ¬åœ°ç¼“å­˜
    config_dir = project_dir.replace("/project", "") + "/config/"  # é…ç½®æ–‡ä»¶

    # åŠ è½½é…ç½®
    config = toml.load(config_dir + "config.toml")
    openai_api_key = config["GPT"]["openai_key"]
    openai_api_base = config["GPT"]["openai_base"]
    kimi_api_key = config["KIMI"]["kimi_key"]
    deepseek_api_key = config["DEEPSEEK"]["deepseek_key"]
    faster_whisper_model = config["WHISPER"]["faster_whisper_model_default"]
    faster_whisper_model_local = config["WHISPER"]["faster_whisper_model_local"]
    faster_whisper_model_path = config["WHISPER"]["faster_whisper_model_local_path"]
    openai_whisper_api = config["WHISPER"]["openai_whisper_api"]

    # é¡µé¢ç¼“å­˜
    st.session_state.openai_base = openai_api_base
    st.session_state.openai_key = openai_api_key
    st.session_state.kimi_key = kimi_api_key
    st.session_state.deepseek_key = deepseek_api_key
    st.session_state.model_local = faster_whisper_model_local
    st.session_state.model_path = faster_whisper_model_path
    st.session_state.faster_whisper_model = faster_whisper_model
    st.session_state.openai_whisper_api = openai_whisper_api

    st.title("AIéŸ³é¢‘è¯†åˆ«ğŸ™ï¸")
    tab1, tab2, tab3 = st.tabs(["éŸ³é¢‘é—®ç­”", "è¯†åˆ«è®¾ç½®", "è¯†åˆ«å†…å®¹"])

    # å¯ç”¨è®¾ç½®
    opt_g = 1
    if "distil" not in faster_whisper_model or torch.cuda.is_available():
        opt_g = 0

    with tab2:
        index = 1
        if openai_whisper_api:
            index = 0
        set_model = st.selectbox("é€‰æ‹©whisperè¯†åˆ«æ¨¡å¼", ("Openai-api æ¥å£è°ƒç”¨", "Faster-whisper æœ¬åœ°éƒ¨ç½²"), index=index)
        st.write('---')
        if set_model == "Faster-whisper æœ¬åœ°éƒ¨ç½²":
            openai_whisper_api = False
            GPU_on = st.toggle('å¯ç”¨GPUåŠ é€Ÿ', disabled=opt_g, help='è‡ªåŠ¨æ£€æµ‹cudaã€pytorchå¯ç”¨åå¼€å¯ï¼')  # GPU
            VAD_on = st.toggle('å¯ç”¨VADè¾…åŠ©', help='å¯ç”¨è¯­éŸ³æ´»åŠ¨æ£€æµ‹ï¼ˆVADï¼‰ä»¥è¿‡æ»¤æ‰æ²¡æœ‰è¯­éŸ³çš„éŸ³é¢‘éƒ¨åˆ†ã€‚')  # VAD
            min_vad = st.number_input('VADé™éŸ³æ£€æµ‹(ms)', min_value=100, max_value=5000, value=500, step=100,
                                      help="å¯ç”¨VADè¾…åŠ©åç”Ÿæ•ˆï¼å¯¹åº”`min_silence_duration_ms`å‚æ•°ï¼Œæœ€å°é™éŸ³æŒç»­æ—¶é—´ã€‚")
            beam_size = st.number_input('æŸæœç´¢å¤§å°', min_value=1, max_value=20, value=5, step=1,
                                        help="`beam_size`å‚æ•°ã€‚ç”¨äºå®šä¹‰æŸæœç´¢ç®—æ³•ä¸­æ¯ä¸ªæ—¶é—´æ­¥ä¿ç•™çš„å€™é€‰é¡¹æ•°é‡ã€‚æŸæœç´¢ç®—æ³•é€šè¿‡åœ¨æ¯ä¸ªæ—¶é—´æ­¥é€‰æ‹©æœ€æœ‰å¯èƒ½çš„å€™é€‰é¡¹æ¥æ„å»ºæœç´¢æ ‘ï¼Œå¹¶æ ¹æ®å€™é€‰é¡¹çš„å¾—åˆ†è¿›è¡Œæ’åºå’Œå‰ªæã€‚è¾ƒå¤§çš„beam_sizeå€¼ä¼šä¿ç•™æ›´å¤šçš„å€™é€‰é¡¹ï¼Œæ‰©å¤§æœç´¢ç©ºé—´ï¼Œå¯èƒ½æé«˜ç”Ÿæˆç»“æœçš„å‡†ç¡®æ€§ï¼Œä½†ä¹Ÿä¼šå¢åŠ è®¡ç®—å¼€é”€ã€‚ç›¸åï¼Œè¾ƒå°çš„beam_sizeå€¼ä¼šå‡å°‘è®¡ç®—å¼€é”€ï¼Œä½†å¯èƒ½å¯¼è‡´æœç´¢è¿‡æ—©åœ°æ”¾å¼ƒæœ€ä½³åºåˆ—ã€‚")
            device = 'cuda' if GPU_on else 'cpu'
            vad = 'True' if VAD_on else 'False'
            language = ('è‡ªåŠ¨è¯†åˆ«', 'zh', 'en', 'ja', 'ko', 'it', 'de')  # language
            lang = st.selectbox('é€‰æ‹©è§†é¢‘è¯­è¨€', language, index=0,
                                help="å¼ºåˆ¶æŒ‡å®šè§†é¢‘è¯­è¨€ä¼šæé«˜è¯†åˆ«å‡†ç¡®åº¦ï¼Œä½†ä¹Ÿå¯èƒ½ä¼šé€ æˆè¯†åˆ«å‡ºé”™ã€‚")
        else:
            openai_whisper_api = True
            proxy_on = st.toggle('å¯ç”¨ä»£ç†', help='å¦‚æœä½ èƒ½ç›´æ¥è®¿é—®openai.comï¼Œåˆ™æ— éœ€å¯ç”¨ã€‚')
            whisper_prompt = st.text_input('Whisperæç¤ºè¯', value='Donâ€™t make each line too long.')
            temperature = st.number_input('Whisperæ¸©åº¦', min_value=0.0, max_value=1.0, value=0.8, step=0.1)

    with (st.sidebar):
        # æ–‡ä»¶ä¸Šä¼ 
        st.write("### æ–‡ä»¶ä¸Šä¼ å™¨")
        uploaded_file = st.file_uploader("è¯·åœ¨è¿™é‡Œä¸Šä¼ éŸ³é¢‘æ–‡ä»¶ï¼š", type=['mp3', 'wav', 'mp4'], label_visibility="collapsed")
        if uploaded_file is not None:  # åˆ¤æ–­æ˜¯å¦ä¸Šä¼ æˆåŠŸ
            st.write("æ–‡ä»¶ç±»å‹:", uploaded_file.type)

        if sac.buttons([sac.ButtonsItem(label='å¯åŠ¨è¯†åˆ«', icon='calendar-week', color='dark')], index=None, align='center', variant='filled', use_container_width=True):
            if uploaded_file is not None:
                with st.spinner('æ­£åœ¨åŠ è½½éŸ³é¢‘ç¼“å­˜...'):
                    current_time = datetime.datetime.now().strftime("%Y-%m-%d %H-%M-%S")
                    output_file = cache_dir + current_time
                    os.makedirs(output_file)
                    if uploaded_file.type == "video/mp4":
                        with open(output_file + "/uploaded.mp4", "wb") as file:
                            file.write(uploaded_file.getbuffer())
                        command = "ffmpeg -i uploaded.mp4 -vn -acodec libmp3lame -ab 320k -f mp3 output.mp3"
                        subprocess.run(command, shell=True, cwd=output_file)
                    else:
                        with open(output_file + "/output.mp3", "wb") as file:
                            file.write(uploaded_file.getbuffer())

                with st.spinner('æ­£åœ¨è¯†åˆ«éŸ³é¢‘å†…å®¹...'):
                    if openai_whisper_api:
                        print("---\nAPIè°ƒç”¨æ¨¡å¼")
                        result = openai_whisper(st.session_state.openai_key, st.session_state.openai_base, proxy_on,
                                                whisper_prompt, temperature, output_file)
                        print("---\nwhisperè¯†åˆ«å†…å®¹ï¼š" + result['text'])
                    else:
                        models_option = st.session_state.faster_whisper_model
                        if st.session_state.model_local:
                            models_option = st.session_state.model_path
                        print("---\næœ¬åœ°è°ƒç”¨æ¨¡å¼\nåŠ è½½æ¨¡å‹ï¼š" + models_option)
                        result = get_whisper_result(uploaded_file, output_file, device, models_option, vad, lang, beam_size,
                                                    min_vad)
                        print("---\nwhisperè¯†åˆ«å†…å®¹ï¼š" + result['text'])

                with st.spinner('æ­£åœ¨ç”Ÿæˆæ–‡æœ¬æ–‡ä»¶...'):
                    srt_content = generate_srt_from_result(result)  # ç”ŸæˆSRTå­—å¹•å†…å®¹
                    with open(output_file + "/output.srt", 'w', encoding='utf-8') as srt_file:  # å°†SRTå†…å®¹å†™å…¥SRTæ–‡ä»¶
                        srt_file.write(srt_content)

                st.session_state.srt_content1 = srt_content
                st.session_state.output = output_file
                st.session_state.text = result['text']
            else:
                st.error("è¯·å…ˆä¸Šä¼ éŸ³é¢‘ï¼")

    with tab1:
        @st.experimental_dialog("è®¾ç½®")
        def setting():
            index_llm = {'moonshot-v1-8k': 0, 'moonshot-v1-32k': 1, 'moonshot-v1-128k': 2,
                         'deepseek-chat': 3, 'gpt-3.5-turbo': 4, 'gpt-4': 5}
            try:
                st.session_state.index = index_llm[st.session_state.translate_option]
            except:
                st.session_state.index = 4
            proxy_on = st.toggle('å¯ç”¨ä»£ç†*', help='å¦‚æœä½ èƒ½ç›´æ¥è®¿é—®openai.comï¼Œåˆ™æ— éœ€å¯ç”¨ã€‚')
            translate_option = st.selectbox('é—®ç­”æ¨¡å‹', (
                'kimi-moonshot-v1-8k', 'kimi-moonshot-v1-32k', 'kimi-moonshot-v1-128k', 'deepseek-v2', 'gpt-3.5-turbo',
                'gpt-4'), index=st.session_state.index)
            prompt = st.text_input('é»˜è®¤æç¤ºè¯', value="ä½ çš„ä»»åŠ¡æ˜¯åŸºäºä»¥ä¸‹éŸ³é¢‘å†…å®¹å’Œè‡ªå·±çš„çŸ¥è¯†å›ç­”ç”¨æˆ·é—®é¢˜ã€‚éŸ³é¢‘å†…å®¹ï¼š")
            temperature = st.number_input('æ¨¡å‹æ¸©åº¦', min_value=0.0, max_value=1.0, value=0.8, step=0.1)
            if 'gpt' in translate_option:
                client = OpenAI(api_key=st.session_state.openai_key)
                if proxy_on:
                    client = OpenAI(api_key=st.session_state.openai_key, base_url=st.session_state.openai_base)
            elif 'kimi' in translate_option:
                translate_option = translate_option.replace('kimi-', '')
                client = OpenAI(api_key=st.session_state.kimi_key, base_url="https://api.moonshot.cn/v1")
            else:
                translate_option = "deepseek-chat"
                client = OpenAI(api_key=st.session_state.deepseek_key, base_url="https://api.deepseek.com/")
            st.session_state.translate_option = translate_option
            st.session_state.client = client
            st.session_state.prompt = prompt
            st.session_state.temperature = temperature

        messages = st.container(height=425)
        if "messages1" not in st.session_state:
            st.session_state["messages1"] = [{"role": "assistant", "content": "æ‚¨å¯¹éŸ³é¢‘å†…å®¹æœ‰ä»€ä¹ˆç–‘é—®?"}]

        for msg1 in st.session_state.messages1:
            messages.chat_message(msg1["role"]).write(msg1["content"])

        try:
            able = False if st.session_state.text else True
        except:
            able = True

        col3, col4 = st.columns([0.95, 0.05])
        with col4:
            if "setting" not in st.session_state:
                if st.button("âš™ï¸", use_container_width=True, help="è¯·å…ˆä¸Šä¼ å¹¶è¿è¡Œç¨‹åºå“¦~"):
                    setting()

        with col3:
            if prompt := st.chat_input(disabled=able, placeholder="åŸºäºéŸ³é¢‘å†…å®¹çš„Chatï¼Œæ‚¨å¯ä»¥é—®ä»»ä½•å…³äºéŸ³é¢‘çš„é—®é¢˜"):
                st.session_state.messages1.append({"role": "user", "content": prompt})
                messages.chat_message("user").write(prompt)
                try:
                    test = st.session_state.client
                except:
                    st.session_state.translate_option = 'gpt-3.5-turbo'
                    st.session_state.client = OpenAI(api_key=st.session_state.openai_key)
                    st.session_state.prompt = 'ä½ çš„ä»»åŠ¡æ˜¯åŸºäºä»¥ä¸‹éŸ³é¢‘å†…å®¹å’Œè‡ªå·±çš„çŸ¥è¯†å›ç­”ç”¨æˆ·é—®é¢˜ã€‚éŸ³é¢‘å†…å®¹ï¼š'
                    st.session_state.temperature = 0.8
                response = st.session_state.client.chat.completions.create(model=st.session_state.translate_option,
                                                                           messages=[{"role": "system",
                                                                                      "content": st.session_state.prompt + st.session_state.text},
                                                                                     {"role": "user", "content": prompt}],
                                                                           temperature=st.session_state.temperature)
                msg1 = response.choices[0].message.content
                st.session_state.messages1.append({"role": "assistant", "content": msg1})
                messages.chat_message("assistant").write(msg1)

        with tab3:
            st.caption("ä»¥ä¸‹å†…å®¹ä¼šåœ¨è¿è¡Œç¨‹åºåè‡ªåŠ¨æ˜¾ç¤ºï¼Œè¯·è¿è¡Œåè€å¿ƒç­‰å¾…ï¼")
            try:
                st.write('##### éŸ³è½¨æ–‡ä»¶ğŸ¶')
                audio_file = open(st.session_state.output + "/output.mp3", 'rb')
                audio_bytes = audio_file.read()
                st.audio(audio_bytes)
            except:
                st.write('')

            try:
                st.write('------')
                st.write('##### å­—å¹•é¢„è§ˆğŸ—’ï¸')
                st.caption("Tipsï¼šæ–‡æœ¬å†…å®¹å¯ä»¥åœ¨å·¦ä¾§è¡¨æ ¼è¿›è¡Œä¿®æ”¹å¾®è°ƒ")
                high = st.slider('æ–‡æœ¬é¢„è§ˆè¡¨æ ¼çš„é«˜åº¦', 100, 1000, 500, 50)
                srt_data = parse_srt_file(st.session_state.srt_content1)
                edited_data = st.data_editor(srt_data, height=high, hide_index=True, use_container_width=True)
                srt = convert_to_srt(edited_data)
                st.download_button(
                    label="ä¸‹è½½ä¿®æ”¹çš„æ–‡ä»¶ï¼ˆSRTæ ¼å¼ï¼‰",
                    data=srt.encode('utf-8'),
                    file_name='output_new.txt'
                )
            except:
                st.write('')
