import os
import toml
import torch
import datetime
import streamlit as st
from openai import OpenAI
from utils.utils import (get_whisper_result, generate_srt_from_result, parse_srt_file, convert_to_srt)

project_dir = os.path.dirname(os.path.abspath(__file__)).replace("\\", "/")
config_dir = project_dir.replace("/pages", "") + "/config/"  # é…ç½®æ–‡ä»¶
cache_dir = project_dir + "/cache/"  # æœ¬åœ°ç¼“å­˜
config = toml.load(config_dir + "config.toml")  # åŠ è½½é…ç½®
st.session_state.openai_key = config["GPT"]["openai_key"]
st.session_state.openai_base = config["GPT"]["openai_base"]

st.set_page_config(
    layout="wide",  # è®¾ç½®å¸ƒå±€æ ·å¼ä¸ºå®½å±•ç¤º
    initial_sidebar_state="expanded"  # è®¾ç½®åˆå§‹è¾¹æ çŠ¶æ€ä¸ºå±•å¼€
)

st.title("AIéŸ³é¢‘è¯†åˆ«ğŸ™ï¸")
col1, col2 = st.columns(2, gap="medium")
with col1:
    # æ–‡ä»¶ä¸Šä¼ 
    uploaded_file = st.file_uploader("è¯·åœ¨è¿™é‡Œä¸Šä¼ éŸ³é¢‘æ–‡ä»¶ï¼š", type=['mp3', 'wav', 'mp4'])
    w_version = st.selectbox('é€‰æ‹©whisperç‰ˆæœ¬', ('openai-whisper', 'faster-whisper'), index=1)
    w_model_option = st.selectbox('é€‰æ‹©è¯†åˆ«æ¨¡å‹', ('tiny', 'base', 'small', 'medium', 'large'), index=4)
    # GPUåŠ é€Ÿ
    wdc = not torch.cuda.is_available()
    GPU_on = st.toggle('å¯ç”¨GPUåŠ é€Ÿ*', disabled=wdc, help='è¯·ç¡®ä¿æ‚¨æ­£ç¡®å®‰è£…äº†cudaã€pytorchï¼Œå¦åˆ™è¯¥é€‰é¡¹æ— æ³•å¼€å¯ï¼')
    device = 'cuda' if GPU_on else 'cpu'
    # VADè¾…åŠ©
    VAD_on = st.toggle('å¯ç”¨VADè¾…åŠ©*', help='è¯·ä½¿ç”¨faster-whisperæ¨¡å‹ï¼Œå¦åˆ™è¯¥é€‰é¡¹æ— æ³•å¼€å¯ï¼')
    vad = 'True' if GPU_on else 'False'

    if st.button('è¿è¡Œç¨‹åº'):
        if uploaded_file is not None:
            with st.spinner('æ­£åœ¨åŠ è½½éŸ³é¢‘ç¼“å­˜...'):
                current_time = datetime.datetime.now().strftime("%Y-%m-%d %H-%M-%S")
                output_file = cache_dir + current_time
                os.makedirs(output_file)
                with open(output_file + "/uploaded.mp3", "wb") as file:
                    file.write(uploaded_file.getbuffer())

            with st.spinner('æ­£åœ¨è¯†åˆ«éŸ³é¢‘å†…å®¹...'):
                result = get_whisper_result(uploaded_file, cache_dir, device, w_model_option, w_version, vad)
                print("whisperè¯†åˆ«ï¼š" + result['text'])

            with st.spinner('æ­£åœ¨ç”ŸæˆSRTå­—å¹•æ–‡ä»¶...'):
                srt_content = generate_srt_from_result(result)  # ç”ŸæˆSRTå­—å¹•å†…å®¹
                with open(output_file + "/output.srt", 'w', encoding='utf-8') as srt_file:  # å°†SRTå†…å®¹å†™å…¥SRTæ–‡ä»¶
                    srt_file.write(srt_content)

            st.session_state.srt_content1 = srt_content
            st.session_state.output = output_file
            st.session_state.text = result['text']
        else:
            st.error("è¯·å…ˆä¸Šä¼ éŸ³é¢‘ï¼")


with col2:
    messages = st.container(height=400)
    if "messages1" not in st.session_state:
        st.session_state["messages1"] = [{"role": "assistant", "content": "æ‚¨å¯¹éŸ³é¢‘å†…å®¹æœ‰ä»€ä¹ˆç–‘é—®?"}]

    for msg1 in st.session_state.messages1:
        messages.chat_message(msg1["role"]).write(msg1["content"])

    try:
        able = False if st.session_state.text else True
    except:
        able = True

    if prompt := st.chat_input(disabled=able, placeholder="åŸºäºéŸ³é¢‘å†…å®¹çš„Chatï¼Œæ‚¨å¯ä»¥é—®ä»»ä½•å…³äºéŸ³é¢‘çš„é—®é¢˜"):
        client = OpenAI(api_key=st.session_state.openai_key)
        st.session_state.messages1.append({"role": "user", "content": prompt})
        messages.chat_message("user").write(prompt)
        response = client.chat.completions.create(model="gpt-3.5-turbo", messages=[{"role": "system", "content": "ä½ çš„ä»»åŠ¡æ˜¯åŸºäºä»¥ä¸‹éŸ³é¢‘å†…å®¹å’Œè‡ªå·±çš„çŸ¥è¯†å›ç­”ç”¨æˆ·é—®é¢˜ã€‚éŸ³é¢‘å†…å®¹ï¼š" + st.session_state.text},
                                                                                   {"role": "user", "content": prompt}])
        msg1 = response.choices[0].message.content
        st.session_state.messages1.append({"role": "assistant", "content": msg1})
        messages.chat_message("assistant").write(msg1)


st.write('------')
st.caption("ä»¥ä¸‹å†…å®¹ä¼šåœ¨è¿è¡Œç¨‹åºåè‡ªåŠ¨æ˜¾ç¤ºï¼Œè¯·è¿è¡Œåè€å¿ƒç­‰å¾…ï¼")
try:
    st.write('##### éŸ³è½¨æ–‡ä»¶ğŸ¶')
    audio_file = open(st.session_state.output + "/uploaded.mp3", 'rb')
    audio_bytes = audio_file.read()
    st.audio(audio_bytes)
except:
    st.write('')

try:
    st.write('------')
    st.write('##### å­—å¹•é¢„è§ˆğŸ—’ï¸')
    st.caption("Tipsï¼šæ–‡æœ¬å†…å®¹å¯ä»¥åœ¨å·¦ä¾§è¡¨æ ¼è¿›è¡Œä¿®æ”¹å¾®è°ƒ")
    high = st.slider('æ–‡æœ¬é¢„è§ˆè¡¨æ ¼çš„é«˜åº¦', 100, 1000, 500, 50)
    col1, col2 = st.columns(2, gap="medium")
    with col1:
        srt_data = parse_srt_file(st.session_state.srt_content1)
        st.dataframe(srt_data, height=high, hide_index=True, use_container_width=True)
        st.download_button(
            label="ä¸‹è½½åŸå§‹çš„æ–‡ä»¶ï¼ˆSRTæ ¼å¼ï¼‰",
            data=st.session_state.srt_content1.encode('utf-8'),
            file_name='output_old.txt'
        )
    with col2:
        edited_data = st.data_editor(srt_data, height=high, hide_index=True, use_container_width=True)
        srt = convert_to_srt(edited_data)
        st.download_button(
            label="ä¸‹è½½ä¿®æ”¹çš„æ–‡ä»¶ï¼ˆSRTæ ¼å¼ï¼‰",
            data=srt.encode('utf-8'),
            file_name='output_new.txt'
        )
except:
    st.write('')