import os
import toml
import requests
import streamlit as st
from styles.global_style import style
from openai import OpenAI


# å‚æ•°é…ç½®
style()
path = os.getcwd() + "/"
config_path = path + "config/llms.toml"
whisper_path = path + "config/whisper.toml"
project_config_path = path + "config/project.toml"
model_path = path + "model/faster-whisper"

with open(config_path, 'r') as config_file:
    llms = toml.load(config_file)
with open(project_config_path, 'r', encoding='utf-8') as config_file:
    project = toml.load(config_file)
with open(whisper_path, 'r', encoding="utf-8") as config_file:
    whispers = toml.load(config_file)

HomeKey = llms["Home"]["key"]
HomeUrl = llms["Home"]["url"]
HomeModel = llms["Home"]["model"]
readme_read = project["other"]["first"]
set_first = whispers["other"]["first"]



parameter_mapping = {
    (0, 1): 'gpt-3.5-turbo',
    (0, 2): 'gpt-4o-mini',
    (0, 3): 'gpt-4',
    (0, 4): 'gpt-4-turbo',
    (0, 5): 'gpt-4o',

    (6, 7): 'claude-3-opus',
    (6, 8): 'claude-3-sonnet',
    (6, 9): 'claude-3-haiku',

    (10, 11): 'gemini-pro',
    (10, 12): 'gemini-1.0-pro',
    (10, 13): 'gemini-1.5-flash',
    (10, 14): 'gemini-1.5-pro',

    (15, 16): 'deepseek-chat',
    (15, 17): 'deepseek-coder',

    (18, 19): 'kimi-moonshot-v1-8k',
    (18, 20): 'kimi-moonshot-v1-32k',
    (18, 21): 'kimi-moonshot-v1-128k',

    (22, 23): 'glm-4',
    (22, 24): 'glm-4-0520',
    (22, 25): 'glm-4-flash',
    (22, 26): 'glm-4-air',
    (22, 27): 'glm-4-airx',

    (28, 29): 'yi-spark',
    (28, 30): 'yi-medium',
    (28, 31): 'yi-medium-200k',
    (28, 32): 'yi-vision',
    (28, 33): 'yi-large',
    (28, 34): 'yi-large-rag',
    (28, 35): 'yi-large-turbo',
    (28, 36): 'yi-large-preview'
}
model = parameter_mapping[tuple(HomeModel)]
st.write("")


@st.dialog("æ¬¢è¿ä½¿ç”¨")
def readme():
    st.write("""
    ## éå¸¸æ„Ÿè°¢æ‚¨æ¥åˆ°æˆ‘çš„ AAVT é¡¹ç›®ï¼
    æœ¬é¡¹ç›®æ—¨åœ¨æä¾›ä¸€ä¸ªç®€å•æ˜“ç”¨çš„è‡ªåŠ¨è¯†åˆ«ã€ç¿»è¯‘å·¥å…·å’Œå…¶ä»–è§†é¢‘è¾…åŠ©å·¥å…·ï¼Œå¸®åŠ©å¿«é€Ÿè¯†åˆ«è§†é¢‘å­—å¹•ã€ç¿»è¯‘å­—å¹•ã€è¾…åŠ©å›¾æ–‡ã€‚
    
    
    
    å¦‚æœæ‚¨éœ€è¦æ›´å¤šå¸®åŠ©ï¼Œå¯ä»¥å‚è€ƒä»¥ä¸‹èµ„æºï¼š
    - ğŸ“˜ [**ç›¸å…³æ•™ç¨‹**](https://blog.chenyme.top/blog/aavt-install)
    - ğŸ“‚ [**é¡¹ç›®åœ°å€**](https://github.com/Chenyme/Chenyme-AAVT)
    - ğŸ’¬ [**äº¤æµç¾¤ç»„**](https://t.me/+j8SNSwhS7xk1NTc9)
    
    æ„Ÿè°¢æ‚¨çš„ä½¿ç”¨ï¼Œ**å¸Œæœ›æ‚¨èƒ½åœ¨ GitHub ä¸Šç»™æˆ‘ä¸€é¢—å…è´¹çš„æ˜Ÿå“Ÿï¼**
    """)
    st.write("")

    if st.button("**æˆ‘å·²çŸ¥æ™“&nbsp;&nbsp;&nbsp;ä¸å†å¼¹å‡º**", type="primary", use_container_width=True, key="blog_first_button"):
        if not set_first:
            with open(whisper_path, 'w', encoding="utf-8") as f:
                whispers["other"]["first"] = True
                whispers["Faster_Local"]["path"] = model_path
                toml.dump(whispers, f)
        with open(project_config_path, 'w', encoding="utf-8") as f:
            project["other"]["first"] = True
            toml.dump(project, f)
        st.session_state.read = True
        st.rerun()
    st.write("")


if not readme_read:
    readme()

if "read" in st.session_state:
    st.toast("æ¬¢è¿ä½¿ç”¨ ~", icon=":material/verified:")
    del st.session_state["read"]
if "stars" not in st.session_state:
    GITHUB_API_URL = "https://api.github.com/repos/Chenyme/Chenyme-AAVT"
    try:
        response = requests.get(GITHUB_API_URL)
        data = response.json()
        st.session_state.stars = data["stargazers_count"]
    except Exception as e:
        st.session_state.stars = ""
        st.toast(f"æ— æ³•è·å–Githubæ•°æ®: {e}")


st.title("Chenyme-AAVT V0.9.0")
st.caption(f" A Project Powered By @Chenyme ğŸŒŸStars {st.session_state.stars}ğŸŒŸ")

st.divider()

context_size = 3

content = """
Python ç‰ˆæœ¬é—®é¢˜ï¼Œç¡®ä¿ä½¿ç”¨ Python 3.8 æˆ–ä»¥ä¸Šç‰ˆæœ¬ï¼Œä»¥é¿å…è¯­æ³•é”™è¯¯ã€‚
ç½‘ç»œé—®é¢˜ï¼Œç½‘ç»œä¸ç¨³å®šæ—¶ï¼Œä½¿ç”¨å›½å†…é•œåƒæºæˆ–æŒ‚ä»£ç†è¿›è¡Œä¾èµ–åº“ä¸‹è½½ã€‚
CMD é—ªé€€æˆ–æŠ¥é”™ï¼Œé‡æ–°å®‰è£… Python å¹¶è®¾ç½® PATH æˆ–æ‰‹åŠ¨ä¿®æ­£ç¯å¢ƒå˜é‡ï¼Œé¿å… CMD é—ªé€€æˆ–çº¢è‰²æŠ¥é”™ã€‚
ä¾èµ–åº“ä¸¢å¤±æˆ–æ–‡ä»¶ç¼ºå¤±ï¼Œç¡®ä¿ä¾èµ–åº“å’Œé¡¹ç›®æ–‡ä»¶å…¨éƒ¨å®‰è£…å’Œæ‹‰å–å®Œæ•´ï¼Œç¼ºå¤±æ—¶é‡æ–°è¿è¡Œå®‰è£…è„šæœ¬æˆ–æ‰‹åŠ¨å®‰è£…ã€‚
XSRF é”™è¯¯ï¼Œç¼–è¾‘é…ç½®æ–‡ä»¶ç¦ç”¨ XSRF ä¿æŠ¤ï¼Œè§£å†³ 403 é”™è¯¯ã€‚
OMP é”™è¯¯ï¼Œåˆ é™¤å¤šä½™çš„ libiomp5md.dll æ–‡ä»¶ï¼Œé¿å… PyTorch åŒ…ä¸­çš„ DLL å†²çªã€‚
é¡¹ç›®æ”¯æŒåŠŸèƒ½
åª’ä½“è¯†åˆ«ï¼šæå–å¹¶ç¿»è¯‘éŸ³è§†é¢‘å­—å¹•
å†…å®¹åŠ©æ‰‹ï¼šé—®ç­”ä¸æ€»ç»“éŸ³è§†é¢‘å†…å®¹
å­—å¹•ç¿»è¯‘ï¼šè‡ªå®šä¹‰å•ä¸ªå­—å¹•ç¿»è¯‘
å›¾æ–‡åšå®¢ï¼šä¸€é”®ç”Ÿæˆå›¾æ–‡åšå®¢
å£°éŸ³æ¨¡æ‹Ÿï¼šæ¨¡æ‹Ÿæ–‡æœ¬å†…å®¹å£°éŸ³
é…ç½®æœ¬åœ°è¯†åˆ«æ¨¡å‹ï¼Œä» Systran ä¸‹è½½æ¨¡å‹æ–‡ä»¶ï¼Œç¡®ä¿åŒ…å« config.jsonã€model.binã€README.mdã€tokenizer.jsonã€vocabulary.txt ç­‰å¿…è¦æ–‡ä»¶ã€‚
æœ¬åœ°è¯†åˆ«æ¨¡å‹å­˜æ”¾ï¼Œå°†ä¸‹è½½çš„æ¨¡å‹æ”¾å…¥é¡¹ç›®ç›®å½• ..\Chenyme_AAVT_0.x.x\models\ ä¸‹çš„è‡ªå®šä¹‰æ–‡ä»¶å¤¹ä¸­ï¼Œç¡®ä¿æ‰€æœ‰å¿…è¦æ–‡ä»¶å®Œæ•´ã€‚
è®¾ç½®ä¸­æ–°å»ºæç¤ºè¯ï¼Œåˆ›å»ºæ–°æç¤ºè¯æ—¶ï¼Œä¿ç•™ {language1} å’Œ {language2} å‚æ•°ï¼Œä»¥åŒ¹é…åŸå§‹è¯­è¨€å’Œç›®æ ‡è¯­è¨€ã€‚
"""


if "messages" not in st.session_state:
    st.session_state["messages"] = [
        {"role": "system", "content": f"ä½ æ˜¯ AAVT é¡¹ç›®çš„ AI åŠ©æ‰‹ï¼Œç”¨æˆ·é—®å…³äºä½ çš„èº«ä»½æ—¶ä¸»åŠ¨å‘ŠçŸ¥ã€‚è¯·å¸®åŠ©ç”¨æˆ·å®Œæˆä»»åŠ¡ï¼Œå¹¶ä¿æŒç®€æ´ã€‚è¿™æ˜¯ä½ å¯èƒ½ç”¨åˆ°çš„çŸ¥è¯†ï¼š{content}ï¼Œå¦‚æœç”¨åˆ°çŸ¥è¯†åº“è¯·ä½¿ç”¨ï¼Œç”¨ä¸åˆ°è¯·è‡ªè¡Œå›ç­”ï¼Œä¸‹é¢è¯·å›ç­”ç”¨æˆ·æé—®ï¼"},
        {"role": "assistant", "content": "æ¬¢è¿ ~ æˆ‘æ˜¯ AAVT é¡¹ç›®çš„ AI åŠ©æ‰‹ï¼(å½“å‰ç‰ˆæœ¬æš‚æœªæ¥å…¥å®Œæ•´çš„é¡¹ç›®æ–‡æ¡£ï¼Œåç»­å†™å®Œä¼šæ”¾å…¥ï¼)"}
    ]

st.write("")
st.write("")

for msg in st.session_state.messages:
    if msg["role"] != "system":
        st.chat_message(msg["role"]).write(msg["content"])

if prompt := st.chat_input(placeholder="NOTEï¼šè¯·æ³¨æ„æ‚¨çš„ Token æ¶ˆè€—å“¦ ~"):
    if not HomeKey or not HomeUrl or not model:
        st.warning(f"**API ç¼ºå¤±å‚æ•°ï¼** \n\n è¯·å‰å¾€ è®¾ç½®-ç¿»è¯‘æ¨¡å‹ ä¸­å…ˆè®¾ç½®æ­¤åŠ©æ‰‹çš„ç›¸å…³å‚æ•°åä½¿ç”¨ï¼", icon=":material/crisis_alert:")
        st.stop()

    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)

    try:
        system_message = st.session_state.messages[0]
        recent_messages = st.session_state.messages[1:][-context_size:]
        recent_messages.insert(0, system_message)

        client = OpenAI(api_key=HomeKey, base_url=HomeUrl)
        response = client.chat.completions.create(model=model, messages=recent_messages, temperature=0.8)

        msg = response.choices[0].message.content
        st.session_state.messages.append({"role": "assistant", "content": msg})
        st.chat_message("assistant").write(msg)
    except Exception as e:
        if "Connection error" in str(e):
            st.error(f"**API è¿æ¥å¤±è´¥ï¼** \n\n Connection errorï¼è¯·æ ¸æŸ¥æ‚¨çš„ ç½‘ç»œè¿æ¥ æˆ–è€… ä»£ç†åœ°å€ æ˜¯å¦æ­£ç¡®å¡«å†™ï¼ŒåŒæ—¶ç¡®ä¿æ‚¨çš„æœåŠ¡å•†æ¥å£æ­£ç¡®æä¾›æ­¤æœåŠ¡ï¼", icon=":material/running_with_errors:")
        else:
            st.error(f"**API è°ƒç”¨å¤±è´¥ï¼** \n\n {e}ï¼", icon=":material/running_with_errors:")


