import time
import json
import subprocess
import requests
from pathlib import Path
from openai import OpenAI
from openai import OpenAIError
from concurrent.futures import ThreadPoolExecutor
from faster_whisper import WhisperModel
import anthropic
import pandas as pd
import google.generativeai as genai
import re
import cv2
import os
import toml
import torch
import base64
import streamlit as st


def get_global_info():
    path = os.getcwd().replace("utils", "") + "/"
    llms_path = path + "config/llms.toml"
    with open(llms_path, 'r', encoding="utf-8") as config_file:
        llms = toml.load(config_file)
    global_key = llms["Global"]["key"]
    global_url = llms["Global"]["url"]
    all_in_one = llms["Global"]["all"]
    return global_key, global_url, all_in_one


def FileToMp3(log_level: str, input_dir: str, output_dir: str, output_name: str = "output.mp3"):
    input_path = Path(input_dir)
    output_path = Path(output_dir) / output_name

    if not input_path.is_file():
        raise FileNotFoundError(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_path}")

    if input_path.suffix.lower() == ".mp3":
        print("\033[1;34mğŸ§ æ–‡ä»¶å·²ç»æ˜¯ MP3 æ ¼å¼ï¼Œæ— éœ€è½¬æ¢ã€‚\033[0m")
        return output_path

    try:
        command = [
            "ffmpeg", "-loglevel", log_level, "-i", str(input_path),
            "-vn", "-acodec", "libmp3lame", "-ab", "320k",
            "-f", "mp3", str(output_path)
        ]
        subprocess.run(command, check=True)
        print("\033[1;34mğŸ§ æ–‡ä»¶å·²æˆåŠŸè½¬æ¢ä¸º MP3 æ ¼å¼ï¼\033[0m")
        return output_path

    except subprocess.CalledProcessError as e:
        raise EOFError(f"FFmpeg æ‰§è¡Œå¤±è´¥: {e}")
    except Exception as e:
        raise EOFError(f"å‘ç”Ÿé”™è¯¯ï¼å¯èƒ½æ˜¯ FFmpeg æœªè¢«æ­£ç¡®é…ç½®æˆ–ä¸Šä¼ æ–‡ä»¶æ ¼å¼ä¸å—æ”¯æŒã€‚è¯¦ç»†ä¿¡æ¯: {e}")


def OpenaiWhisperResult(key: str, url: str, path: str, model: str, prompt: str, temperature: float):
    print("\n\033[1;35m*** OpenAI Whisper API è°ƒç”¨æ¨¡å¼ ***\033[0m\n")
    print(f"\033[1;34mğŸˆ´ è¯†åˆ«æ¨¡å‹: {model}\033[0m")
    global_key, global_url, all_in_one = get_global_info()

    if all_in_one:
        client = OpenAI(api_key=global_key, base_url=global_url)
        print(f"\033[1;34mğŸŒ ä»£ç†åœ°å€: {global_url}\033[0m")
    else:
        client = OpenAI(api_key=key, base_url=url)
        if url != "https://api.openai.com/v1":
            print(f"\033[1;34mğŸŒ ä»£ç†åœ°å€: {url}\033[0m")

    audio_path = Path(path)
    try:
        if audio_path.is_file():
            audio_file_path = audio_path
        else:
            audio_file_path = audio_path / "output.mp3"
        if not audio_file_path.is_file():
            raise FileNotFoundError(f"æ–‡ä»¶æœªæ‰¾åˆ°ï¼š{audio_file_path}")
    except FileNotFoundError as e:
        return {"error": f"æ–‡ä»¶é”™è¯¯ï¼š{str(e)}"}

    try:
        with audio_file_path.open("rb") as audio_file:
            transcript = client.audio.transcriptions.create(
                model=model,
                file=audio_file,
                response_format="verbose_json",
                timestamp_granularities=["segment"],
                prompt=prompt,
                temperature=temperature
            )
    except OpenAIError as e:
        return {"error": f"OpenAI Whisper API è¯·æ±‚å¤±è´¥ï¼š{str(e)}"}

    finally:
        audio_file.close()

    result = {'text': transcript.text, 'segments': transcript.segments}
    print(f"\033[1;34mğŸ“ Whisperè¯†åˆ«ç»“æœ:\033[0m\n{result['text']}\n")
    return result


def FasterWhisperResultDict(segments):
    segments = list(segments)
    return {
        'text': ' '.join([segment.text for segment in segments]),
        'segments': [{
            'id': segment.id,
            'seek': segment.seek,
            'start': segment.start,
            'end': segment.end,
            'text': segment.text,
            'tokens': segment.tokens,
            'temperature': segment.temperature,
            'avg_logprob': segment.avg_logprob,
            'compression_ratio': segment.compression_ratio,
            'no_speech_prob': segment.no_speech_prob}
            for segment in segments
        ]
    }


def FasterWhisperResult(file_path: str, device: str, model_name: str, prompt: str, temp: float, vad: bool, lang: str, beam_size: int, min_vad: int):
    valid_models = ['tiny', 'tiny.en', 'base', 'base.en', 'small', 'small.en', 'medium', 'medium.en', 'large-v1', 'large-v2', 'large-v3', 'large', 'distil-small.en', 'distil-medium.en', 'distil-large-v2', 'distil-large-v3']
    if device:
        device = "cuda"
    else:
        device = "cpu"

    if model_name in valid_models:
        print("\n\033[1;35m*** Faster Whisper è‡ªåŠ¨ä¸‹è½½æ¨¡å¼ ***\033[0m\n")
    else:
        print("\n\033[1;35m*** Faster Whisper æœ¬åœ°æ¨¡å‹æ¨¡å¼ ***\033[0m\n")

    print(f"\033[1;34mğŸ–¥ï¸ è¿è¡Œæ¨¡å‹: {model_name} \033[0m")
    print(f"\033[1;34mâš™ï¸ï¸ è¿è¡Œæ–¹å¼: {device} \033[0m")
    print(f"\033[1;34mğŸ™ï¸ VADè¾…åŠ©: {vad} \033[0m")

    file_path = Path(file_path)
    try:
        if file_path.is_file():
            audio_file = file_path.open("rb")
        else:
            audio_file = (file_path / "output.mp3").open("rb")
    except FileNotFoundError as e:
        return {"error": f"æ–‡ä»¶é”™è¯¯ï¼š{str(e)}"}

    try:
        with audio_file:
            model = WhisperModel(model_name, device)
            transcribe_args = {
                "initial_prompt": prompt,
                "beam_size": beam_size,
                "temperature": temp
            }
            if vad:
                transcribe_args.update({
                    "vad_filter": vad,
                    "vad_parameters": dict(min_silence_duration_ms=min_vad)
                })
            if lang != "è‡ªåŠ¨è¯†åˆ«":
                transcribe_args["language"] = lang

            segments, _ = model.transcribe(audio_file, **transcribe_args)

        result = FasterWhisperResultDict(segments)
        print(f"\033[1;34mğŸ“ Whisperè¯†åˆ«ç»“æœ:\033[0m\n{result['text']}")
        return result

    except Exception as e:
        return {"error": f"è½¬å½•è¿‡ç¨‹å‘ç”Ÿé”™è¯¯ï¼š{str(e)}"}


def runWhisperSeperateProc(*args):
    try:
        with ThreadPoolExecutor() as executor:
            future = executor.submit(FasterWhisperResult, *args)
            return future.result()

    except Exception as e:
        st.error(f"An error occurred during transcription: {e}")
        st.stop()


def translate(system_prompt, user_prompt, api_key, base_url, model, result, wait_time, srt):
    print("\n\033[1;35m*** API è°ƒç”¨ç¿»è¯‘æ¨¡å¼ ***\033[0m\n")
    print(f"\033[1;34mğŸˆ´ ç¿»è¯‘æ¨¡å‹: {model}\033[0m")
    global_key, global_url, all_in_one = get_global_info()

    if all_in_one:
        client = OpenAI(api_key=global_key, base_url=global_url)
        print(f"\033[1;34mğŸŒ ä»£ç†åœ°å€: {global_url}\033[0m")
        print(f"\033[1;34mğŸ“ ç¿»è¯‘å†…å®¹: \033[0m")

        segment_id = 0
        segments = result['segments']

        context_size = 2
        previous_texts = []
        original_user_prompt = user_prompt

        for segment in segments:
            text = segment['text']

            if len(previous_texts) > 0:
                combined_text = "\n".join(previous_texts[-context_size:]) + "\n"
            else:
                combined_text = text
            current_user_prompt = original_user_prompt.replace("{combined_text}", combined_text)

            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": current_user_prompt + str(text)}
                ])
            answer = response.choices[0].message.content

            if srt == "åŸå§‹è¯­è¨€ä¸ºé¦–":
                result['segments'][segment_id]['text'] = str(text) + "\n" + str(answer)
            elif srt == "ç›®æ ‡è¯­è¨€ä¸ºé¦–":
                result['segments'][segment_id]['text'] = str(answer) + "\n" + str(text)
            else:
                result['segments'][segment_id]['text'] = answer
            segment_id += 1
            print(answer)

            previous_texts.append(text)
            if len(previous_texts) > context_size:
                previous_texts.pop(0)
            time.sleep(wait_time)

    else:
        if "gpt" in model:
            if base_url != "https://api.openai.com/v1":
                print(f"\033[1;34mğŸŒ ä»£ç†åœ°å€: {base_url}\033[0m")
            print(f"\033[1;34mğŸ“ ç¿»è¯‘å†…å®¹: \033[0m")
            client = OpenAI(api_key=api_key, base_url=base_url)
            segment_id = 0
            segments = result['segments']
            original_user_prompt = user_prompt
            context_size = 2
            previous_texts = []
            for segment in segments:
                text = segment['text']

                if len(previous_texts) > 0:
                    combined_text = "\n".join(previous_texts[-context_size:]) + "\n"
                else:
                    combined_text = text
                current_user_prompt = original_user_prompt.replace("{combined_text}", combined_text)
                response = client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": current_user_prompt + str(text)}
                    ])
                answer = response.choices[0].message.content

                if srt == "åŸå§‹è¯­è¨€ä¸ºé¦–":
                    result['segments'][segment_id]['text'] = str(text) + "\n" + str(answer)
                elif srt == "ç›®æ ‡è¯­è¨€ä¸ºé¦–":
                    result['segments'][segment_id]['text'] = str(answer) + "\n" + str(text)
                else:
                    result['segments'][segment_id]['text'] = answer

                segment_id += 1
                print(answer)

                previous_texts.append(text)
                if len(previous_texts) > context_size:
                    previous_texts.pop(0)

                time.sleep(wait_time)

        elif "claude" in model:
            if base_url != "https://api.anthropic.com/v1/messages":
                print(f"\033[1;34mğŸŒ ä»£ç†åœ°å€: {base_url}\033[0m")
            print(f"\033[1;34mğŸ“ ç¿»è¯‘å†…å®¹: \033[0m")
            segment_id = 0
            segments = result['segments']
            client = anthropic.Anthropic(api_key=api_key, base_url=base_url)

            context_size = 2
            previous_texts = []
            original_user_prompt = user_prompt

            for segment in segments:
                text = segment['text']

                if len(previous_texts) > 0:
                    combined_text = "\n".join(previous_texts[-context_size:]) + "\n"
                else:
                    combined_text = text
                current_user_prompt = original_user_prompt.replace("{combined_text}", combined_text)

                message = client.messages.create(
                    model=model,
                    max_tokens=1024,
                    system=system_prompt,
                    messages=[
                        {"role": "user", "content": current_user_prompt + str(text)}
                    ])

                answer = message.content[0]['text']
                if srt == "åŸå§‹è¯­è¨€ä¸ºé¦–":
                    result['segments'][segment_id]['text'] = str(text) + "\n" + str(answer)
                elif srt == "ç›®æ ‡è¯­è¨€ä¸ºé¦–":
                    result['segments'][segment_id]['text'] = str(answer) + "\n" + str(text)
                else:
                    result['segments'][segment_id]['text'] = answer
                segment_id += 1
                print(answer)

                previous_texts.append(text)
                if len(previous_texts) > context_size:
                    previous_texts.pop(0)

                time.sleep(wait_time)

        elif "gemini" in model:
            if base_url is None:
                print("- Python SDK è°ƒç”¨ï¼šå¦‚æœéœ€è¦ä½¿ç”¨ä»£ç†ï¼Œè¯·å¡«å†™ BASE_URLï¼Œç³»ç»Ÿå°†è‡ªåŠ¨å¯ç”¨ä»£ç†æ¨¡å¼ï¼")
                segment_id = 0
                segments = result['segments']

                context_size = 2
                previous_texts = []
                original_user_prompt = user_prompt

                print(f"\033[1;34mğŸ“ ç¿»è¯‘å†…å®¹: \033[0m")
                for segment in segments:
                    text = segment['text']

                    if len(previous_texts) > 0:
                        combined_text = "\n".join(previous_texts[-context_size:]) + "\n"
                    else:
                        combined_text = text
                    current_user_prompt = original_user_prompt.replace("{combined_text}", combined_text)

                    genai.configure(api_key=api_key)
                    model = genai.GenerativeModel(model)
                    answer = model.generate_content(system_prompt + current_user_prompt + str(text))
                    if srt == "åŸå§‹è¯­è¨€ä¸ºé¦–":
                        result['segments'][segment_id]['text'] = str(text) + "\n" + str(answer)
                    elif srt == "ç›®æ ‡è¯­è¨€ä¸ºé¦–":
                        result['segments'][segment_id]['text'] = str(answer) + "\n" + str(text)
                    else:
                        result['segments'][segment_id]['text'] = answer
                    segment_id += 1
                    print(answer.text)

                    previous_texts.append(text)
                    if len(previous_texts) > context_size:
                        previous_texts.pop(0)

                    time.sleep(wait_time)
            else:
                print("- Request è¯·æ±‚è°ƒç”¨ï¼ˆé€‚ç”¨äºä»£ç†æ¨¡å¼ï¼Œè‹¥ä¸æƒ³ä½¿ç”¨Requestæ¨¡å¼ï¼Œè¯·æŠŠBASE_URLç•™ç©ºï¼)")
                print(f"\033[1;34mğŸŒ ä»£ç†åœ°å€: {base_url}\033[0m")
                print(f"\033[1;34mğŸ“ ç¿»è¯‘å†…å®¹: \033[0m")
                segment_id = 0
                segments = result['segments']

                context_size = 2
                previous_texts = []
                original_user_prompt = user_prompt

                for segment in segments:
                    text = segment['text']

                    if len(previous_texts) > 0:
                        combined_text = "\n".join(previous_texts[-context_size:]) + "\n"
                    else:
                        combined_text = text
                    current_user_prompt = original_user_prompt.replace("{combined_text}", combined_text)

                    payload = json.dumps({
                        "model": model,
                        "messages": [
                            {
                                "role": "system",
                                "content": system_prompt
                            },
                            {
                                "role": "user",
                                "content": current_user_prompt + str(text)
                            }
                        ],
                        "temperature": 0.8
                    })
                    headers = {
                        'Content-Type': 'application/json',
                        'Authorization': api_key
                    }
                    response = requests.request("POST", base_url, headers=headers, data=payload)
                    answer = response.json()
                    try:
                        answer = answer["choices"][0]["message"]["content"]
                        if srt == "åŸå§‹è¯­è¨€ä¸ºé¦–":
                            result['segments'][segment_id]['text'] = str(text) + "\n" + str(answer)
                        elif srt == "ç›®æ ‡è¯­è¨€ä¸ºé¦–":
                            result['segments'][segment_id]['text'] = str(answer) + "\n" + str(text)
                        else:
                            result['segments'][segment_id]['text'] = answer
                        segment_id += 1
                        print(answer.text)

                        previous_texts.append(text)
                        if len(previous_texts) > context_size:
                            previous_texts.pop(0)

                        time.sleep(wait_time)
                    except Exception as e:
                        print("\n\033[1;31m------- è¯·é˜…è¯»ä¸‹æ–¹æŠ¥é”™å†…å®¹ -------\033[0m\n")
                        print(f"\033[1;31mâŒ å‘ç”Ÿé”™è¯¯: {type(e).__name__}\033[0m")
                        print(f"\033[1;33mğŸ’¬ é”™è¯¯è¯¦æƒ…: \033[0m\n{answer}\n")
                        print("\033[1;31m------- è¯·æ ¹æ®æŠ¥é”™å†…å®¹æ£€æŸ¥ä½ çš„ API Key æˆ–ä»£ç†é…ç½® -------\033[0m\n")
                        raise e

        else:
            print(f"\033[1;34mğŸ“ ç¿»è¯‘å†…å®¹: \033[0m")
            client = OpenAI(api_key=api_key, base_url=base_url)
            segment_id = 0
            segments = result['segments']

            context_size = 2
            previous_texts = []
            original_user_prompt = user_prompt

            for segment in segments:
                text = segment['text']

                if len(previous_texts) > 0:
                    combined_text = "\n".join(previous_texts[-context_size:]) + "\n"
                else:
                    combined_text = text
                current_user_prompt = original_user_prompt.replace("{combined_text}", combined_text)

                response = client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": current_user_prompt + str(text)}
                    ])
                answer = response.choices[0].message.content

                if srt == "åŸå§‹è¯­è¨€ä¸ºé¦–":
                    result['segments'][segment_id]['text'] = str(text) + "\n" + str(answer)
                elif srt == "ç›®æ ‡è¯­è¨€ä¸ºé¦–":
                    result['segments'][segment_id]['text'] = str(answer) + "\n" + str(text)
                else:
                    result['segments'][segment_id]['text'] = answer
                segment_id += 1
                print(answer)

                previous_texts.append(text)
                if len(previous_texts) > context_size:
                    previous_texts.pop(0)

                time.sleep(wait_time)
    return result


def local_translate(system_prompt, user_prompt, api_key, base_url, model, result, srt):
    print("\n\033[1;35m*** æœ¬åœ°å¤§æ¨¡å‹ç¿»è¯‘æ¨¡å¼ ***\033[0m\n")
    print(f"\033[1;34mğŸˆ´ ç¿»è¯‘æ¨¡å‹: {model}\033[0m")
    print(f"\033[1;34mğŸ“ ç¿»è¯‘å†…å®¹: \033[0m")
    client = OpenAI(api_key=api_key, base_url=base_url)
    segments = result['segments']
    segment_id = 0

    context_size = 2
    previous_texts = []
    original_user_prompt = user_prompt

    for segment in segments:
        text = segment['text']

        if len(previous_texts) > 0:
            combined_text = "\n".join(previous_texts[-context_size:]) + "\n"
        else:
            combined_text = text
        current_user_prompt = original_user_prompt.replace("{combined_text}", combined_text)

        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": current_user_prompt + str(text)}
            ])

        answer = response.choices[0].message.content
        if srt == "åŸå§‹è¯­è¨€ä¸ºé¦–":
            result['segments'][segment_id]['text'] = str(text) + "\n" + str(answer)
        elif srt == "ç›®æ ‡è¯­è¨€ä¸ºé¦–":
            result['segments'][segment_id]['text'] = str(answer) + "\n" + str(text)
        else:
            result['segments'][segment_id]['text'] = answer
        segment_id += 1
        print(answer)

        previous_texts.append(text)
        if len(previous_texts) > context_size:
            previous_texts.pop(0)

    return result


def milliseconds_to_srt_time_format(milliseconds):  # å°†æ¯«ç§’è¡¨ç¤ºçš„æ—¶é—´è½¬æ¢ä¸ºSRTå­—å¹•çš„æ—¶é—´æ ¼å¼
    seconds, milliseconds = divmod(milliseconds, 1000)
    minutes, seconds = divmod(seconds, 60)
    hours, minutes = divmod(minutes, 60)
    return f"{hours:02d}:{minutes:02d}:{seconds:02d},{milliseconds:03d}"


def generate_srt_from_result(result):  # æ ¼å¼åŒ–ä¸ºSRTå­—å¹•çš„å½¢å¼
    segments = result['segments']
    srt_content = ''
    segment_id = 1
    for segment in segments:
        start_time = int(segment['start'] * 1000)
        end_time = int(segment['end'] * 1000)
        text = segment['text']

        srt_content += f"{segment_id}\n"
        srt_content += f"{milliseconds_to_srt_time_format(start_time)} --> {milliseconds_to_srt_time_format(end_time)}\n"
        srt_content += f"{text}\n\n"
        segment_id += 1
    return srt_content


def generate_srt_from_result_2(result, font, font_size, font_color):  # æ ¼å¼åŒ–ä¸ºSRTå­—å¹•çš„å½¢å¼
    segments = result['segments']
    srt_content = ''
    segment_id = 1
    for segment in segments:
        start_time = int(segment['start'] * 1000)
        end_time = int(segment['end'] * 1000)
        text = segment['text']

        srt_content += f"{segment_id}\n"
        srt_content += f"{milliseconds_to_srt_time_format(start_time)} --> {milliseconds_to_srt_time_format(end_time)}\n"
        srt_content += f"<font color={font_color}><font face={font}><font size={font_size}> {text}\n\n"
        segment_id += 1
    return srt_content


def check_cuda_installed():
    if torch.cuda.is_available():
        return True
    else:
        return False

def check_ffmpeg_hwaccel():
    try:
        result = subprocess.run(["ffmpeg", "-hwaccels"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        hwaccels = result.stdout.splitlines()
        if 'cuda' in hwaccels:
            return True
        else:
            return False
    except Exception as e:
        return False


def srt_mv(log, name, crf, quality, setting, path, font, font_size, font_color, subtitle_model):  # è§†é¢‘åˆæˆå­—å¹•
    font_color = font_color.lstrip('#')  # å»æ‰ '#' ç¬¦å·
    bb = font_color[4:6]
    gg = font_color[2:4]
    rr = font_color[0:2]
    font_color = f"&H{bb}{gg}{rr}&"
    cuda_installed = check_cuda_installed()
    cuda_supported = check_ffmpeg_hwaccel() if cuda_installed else False

    if subtitle_model == "ç¡¬å­—å¹•":
        if cuda_supported:
            command = f"""ffmpeg -loglevel {log} -hwaccel cuda -i {name} -lavfi "subtitles=output.srt:force_style='FontName={font},FontSize={font_size},PrimaryColour={font_color},Outline=1,Shadow=0,BackColour=&H9C9C9C&,Bold=-1,Alignment=2'" -preset {quality} -c:v {setting} -crf {crf} -y -c:a copy output.mp4"""
        else:
            command = f"""ffmpeg -loglevel {log} -i {name} -lavfi "subtitles=output.srt:force_style='FontName={font},FontSize={font_size},PrimaryColour={font_color},Outline=1,Shadow=0,BackColour=&H9C9C9C&,Bold=-1,Alignment=2'" -preset {quality} -c:v libx264 -crf {crf} -y -c:a copy output.mp4"""
    else:
        if cuda_supported:
            command = f"""ffmpeg -loglevel {log} -hwaccel cuda -i {name} -i output_with_style.srt -c:v {setting} -crf {crf} -y -c:a copy -c:s mov_text -preset {quality} output.mp4"""
        else:
            command = f"""ffmpeg -loglevel {log} -i {name} -i output_with_style.srt -c:v libx264 -crf {crf} -y -c:a copy -c:s mov_text -preset {quality} output.mp4"""

    subprocess.run(command, shell=True, cwd=path)


def parse_srt_file(srt_content, srt_setting="å…³é—­"):
    lines = srt_content.strip().split('\n')
    subtitles = []

    if srt_setting == "å…³é—­":
        current_subtitle = {'ç´¢å¼•': '', 'èµ·å§‹': '', 'ç»“æŸ': '', 'å­—å¹•': ''}
    else:
        current_subtitle = {'ç´¢å¼•': '', 'èµ·å§‹': '', 'ç»“æŸ': '', 'å­—å¹•': '', 'ç¿»è¯‘å­—å¹•': ''}

    for line in lines:
        line = line.strip()

        if line.isdigit():
            if current_subtitle['ç´¢å¼•']:
                subtitles.append(current_subtitle)
            if srt_setting == "å…³é—­":
                current_subtitle = {'ç´¢å¼•': str(line), 'èµ·å§‹': '', 'ç»“æŸ': '', 'å­—å¹•': ''}
            else:
                current_subtitle = {'ç´¢å¼•': str(line), 'èµ·å§‹': '', 'ç»“æŸ': '', 'å­—å¹•': '', 'ç¿»è¯‘å­—å¹•': ''}
        elif '-->' in line:
            start_time, end_time = line.split('-->')
            current_subtitle['èµ·å§‹'] = start_time.strip()
            current_subtitle['ç»“æŸ'] = end_time.strip()
        elif line != '':
            if srt_setting == "å…³é—­":
                current_subtitle['å­—å¹•'] += line + ' '
            elif srt_setting == "åŸå§‹è¯­è¨€ä¸ºé¦–":
                if not current_subtitle['å­—å¹•']:
                    current_subtitle['å­—å¹•'] = line
                else:
                    current_subtitle['ç¿»è¯‘å­—å¹•'] = line
            elif srt_setting == "ç›®æ ‡è¯­è¨€ä¸ºé¦–":
                if not current_subtitle['ç¿»è¯‘å­—å¹•']:
                    current_subtitle['ç¿»è¯‘å­—å¹•'] = line
                else:
                    current_subtitle['å­—å¹•'] = line

    if current_subtitle['ç´¢å¼•']:
        subtitles.append(current_subtitle)

    df = pd.DataFrame(subtitles)
    return df


def convert_to_srt(edited_data, srt_setting="å…³é—­"):
    subtitles = ''
    for index, row in edited_data.iterrows():
        start_time = row['èµ·å§‹']
        end_time = row['ç»“æŸ']

        if srt_setting == "å…³é—­":
            content = row.get('å­—å¹•', '')
            subtitle = f"{index + 1}\n{start_time} --> {end_time}\n{content.strip()}\n\n"
        elif srt_setting == "åŸå§‹è¯­è¨€ä¸ºé¦–":
            content1 = row.get('å­—å¹•', '')
            content2 = row.get('ç¿»è¯‘å­—å¹•', '')
            subtitle = f"{index + 1}\n{start_time} --> {end_time}\n{content1}\n{content2}\n\n"
        elif srt_setting == "ç›®æ ‡è¯­è¨€ä¸ºé¦–":
            content1 = row.get('å­—å¹•', '')
            content2 = row.get('ç¿»è¯‘å­—å¹•', '')
            subtitle = f"{index + 1}\n{start_time} --> {end_time}\n{content2}\n{content1}\n\n"

        subtitles += subtitle
    return subtitles


def show_video(path, name):
    video_file = open(path + "/" + name, 'rb')
    video_bytes = video_file.read()
    return video_bytes


def add_font_settings(srt_content, font_color, font_face, font_size, srt_setting="å…³é—­"):
    timestamp_pattern = re.compile(r"(\d{2}:\d{2}:\d{2},\d{3} --> \d{2}:\d{2}:\d{2},\d{3})")
    lines = srt_content.split("\n")
    result = []
    is_translation = False

    for line in lines:
        if line.isdigit():
            result.append(line)
            is_translation = False
        elif timestamp_pattern.match(line):
            result.append(line)
            is_translation = False
        elif line.strip() == "":
            result.append(line)
            is_translation = False
        else:
            if srt_setting == "å…³é—­":
                formatted_line = f'<font color="{font_color}" face="{font_face}" size="{font_size}">{line}</font>'
                result.append(formatted_line)
            elif srt_setting == "åŸå§‹è¯­è¨€ä¸ºé¦–":
                if not is_translation:
                    formatted_line = f'<font color="{font_color}" face="{font_face}" size="{font_size}">{line}</font>'
                    is_translation = True
                else:
                    formatted_line = f'<font color="{font_color}" face="{font_face}" size="{font_size}">{line}</font>'
                result.append(formatted_line)
            elif srt_setting == "ç›®æ ‡è¯­è¨€ä¸ºé¦–":
                if not is_translation:
                    formatted_line = f'<font color="{font_color}" face="{font_face}" size="{font_size}">{line}</font>'
                    is_translation = True
                else:
                    formatted_line = f'<font color="{font_color}" face="{font_face}" size="{font_size}">{line}</font>'
                result.append(formatted_line)
    return "\n".join(result)


def srt_to_vtt(srt_content):

    blocks = srt_content.strip().split('\n\n')
    vtt_lines = ['WEBVTT\n']

    for block in blocks:
        lines = block.strip().split('\n')

        if len(lines) >= 2:
            time_range = lines[1].replace(',', '.')
            text = '\n'.join(lines[2:])
            vtt_lines.append(f'{time_range}\n{text}\n')
    vtt_content = '\n'.join(vtt_lines)

    return vtt_content


def srt_to_ass(srt_content, fontname="Arial", size=20, color="&H00FFFFFF"):

    lines = srt_content.strip().split('\n\n')
    ass_header = (
        '[Script Info]\n'
        'Title: Converted from SRT\n'
        'ScriptType: v4.00+\n'
        'Collisions: Normal\n'
        'PlayDepth: 0\n\n'
        '[V4+ Styles]\n'
        'Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, Bold, Italic, BorderStyle, Outline, Shadow, Alignment, MarginL, MarginR, MarginV, Encoding\n'
        f'Style: Default,{fontname},{size},{color},&H000000FF,&H00000000,-1,0,0,1,1.0,0.0,2,10,10,10,1\n\n'
        '[Events]\n'
        'Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text\n'
    )
    ass_content = ass_header

    for line in lines:
        parts = line.strip().split('\n')
        start, end = parts[1].split(' --> ')
        start = start.replace(',', '.')
        end = end.replace(',', '.')
        text = '\\N'.join(parts[2:])
        ass_content += f'Dialogue: 0,{start},{end},Default,,0,0,0,,{text}\n'

    return ass_content


def srt_to_sbv(srt_content):

    lines = srt_content.strip().split('\n\n')
    sbv_content = ''

    for block in lines:
        parts = block.strip().split('\n')
        start, end = parts[1].split(' --> ')
        start = convert_srt_time_to_sbv(start)
        end = convert_srt_time_to_sbv(end)
        text = '\n'.join(parts[2:])
        sbv_content += f'{start},{end}\n{text}\n\n'

    return sbv_content


def convert_srt_time_to_sbv(time_str):

    hours, minutes, seconds = time_str.split(':')
    seconds, milliseconds = seconds.split(',')
    milliseconds = int(milliseconds) / 10
    sbv_time = f'{hours}:{minutes}:{seconds}.{int(milliseconds):02d}'

    return sbv_time


def local_translate_srt(system_prompt, user_prompt, api_key, base_url, model, srt_content, srt):
    print("\n\033[1;35m*** æœ¬åœ°å¤§æ¨¡å‹ç¿»è¯‘æ¨¡å¼ ***\033[0m\n")
    print(f"\033[1;34mğŸˆ´ ç¿»è¯‘æ¨¡å‹: {model}\033[0m")
    print(f"\033[1;34mğŸ“ ç¿»è¯‘å†…å®¹: \033[0m")
    client = OpenAI(api_key=api_key, base_url=base_url)
    segment_id = 0

    context_size = 2
    previous_texts = []
    original_user_prompt = user_prompt

    for segment in srt_content:
        text = segment['text']

        if len(previous_texts) > 0:
            combined_text = "\n".join(previous_texts[-context_size:]) + "\n"
        else:
            combined_text = text
        current_user_prompt = original_user_prompt.replace("{combined_text}", combined_text)

        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": current_user_prompt + str(text)}
            ])
        answer = response.choices[0].message.content
        if srt == "åŸå§‹è¯­è¨€ä¸ºé¦–":
            srt_content[segment_id]['text'] = str(text) + "\n" + str(answer)
        elif srt == "ç›®æ ‡è¯­è¨€ä¸ºé¦–":
            srt_content[segment_id]['text'] = str(answer) + "\n" + str(text)
        else:
            srt_content[segment_id]['text'] = answer
        segment_id += 1
        print(answer)

        previous_texts.append(text)
        if len(previous_texts) > context_size:
            previous_texts.pop(0)

    return srt_content


def translate_srt(system_prompt, user_prompt, api_key, base_url, model, srt_content, wait_time, srt):
    print("\n\033[1;35m*** API è°ƒç”¨ç¿»è¯‘æ¨¡å¼ ***\033[0m\n")
    print(f"\033[1;34mğŸˆ´ ç¿»è¯‘æ¨¡å‹: {model}\033[0m")
    global_key, global_url, all_in_one = get_global_info()
    if all_in_one:
        print(f"\033[1;34mğŸ“ ç¿»è¯‘å†…å®¹: \033[0m")
        print(f"\033[1;34mğŸŒ ä»£ç†åœ°å€: {base_url}\033[0m")
        client = OpenAI(api_key=global_key, base_url=global_url)

        segment_id = 0
        context_size = 2
        previous_texts = []
        original_user_prompt = user_prompt

        for segment in srt_content:
            text = segment['text']

            if len(previous_texts) > 0:
                combined_text = "\n".join(previous_texts[-context_size:]) + "\n"
            else:
                combined_text = text
            current_user_prompt = original_user_prompt.replace("{combined_text}", combined_text)

            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": current_user_prompt + str(text)}
                ])
            answer = response.choices[0].message.content
            if srt == "åŸå§‹è¯­è¨€ä¸ºé¦–":
                srt_content[segment_id]['text'] = str(text) + "\n" + str(answer)
            elif srt == "ç›®æ ‡è¯­è¨€ä¸ºé¦–":
                srt_content[segment_id]['text'] = str(answer) + "\n" + str(text)
            else:
                srt_content[segment_id]['text'] = answer
            segment_id += 1
            print(answer)

            previous_texts.append(text)
            if len(previous_texts) > context_size:
                previous_texts.pop(0)

            time.sleep(wait_time)
    else:
        if "gpt" in model:
            if base_url != "https://api.openai.com/v1":
                print(f"\033[1;34mğŸŒ ä»£ç†åœ°å€: {base_url}\033[0m")
            print(f"\033[1;34mğŸ“ ç¿»è¯‘å†…å®¹: \033[0m")
            client = OpenAI(api_key=api_key, base_url=base_url)
            segment_id = 0

            context_size = 2
            previous_texts = []
            original_user_prompt = user_prompt

            for segment in srt_content:
                text = segment['text']

                if len(previous_texts) > 0:
                    combined_text = "\n".join(previous_texts[-context_size:]) + "\n"
                else:
                    combined_text = text
                current_user_prompt = original_user_prompt.replace("{combined_text}", combined_text)

                response = client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": current_user_prompt + str(text)}
                    ])
                answer = response.choices[0].message.content

                if srt == "åŸå§‹è¯­è¨€ä¸ºé¦–":
                    srt_content[segment_id]['text'] = str(text) + "\n" + str(answer)
                elif srt == "ç›®æ ‡è¯­è¨€ä¸ºé¦–":
                    srt_content[segment_id]['text'] = str(answer) + "\n" + str(text)
                else:
                    srt_content[segment_id]['text'] = answer
                segment_id += 1
                print(answer)

                previous_texts.append(text)
                if len(previous_texts) > context_size:
                    previous_texts.pop(0)

                time.sleep(wait_time)

        elif "claude" in model:
            if base_url != "https://api.anthropic.com/v1/messages":
                print(f"\033[1;34mğŸŒ ä»£ç†åœ°å€: {base_url}\033[0m")
            print(f"\033[1;34mğŸ“ ç¿»è¯‘å†…å®¹: \033[0m")
            segment_id = 0

            context_size = 2
            previous_texts = []
            original_user_prompt = user_prompt

            for segment in srt_content:
                text = segment['text']

                if len(previous_texts) > 0:
                    combined_text = "\n".join(previous_texts[-context_size:]) + "\n"
                else:
                    combined_text = text
                current_user_prompt = original_user_prompt.replace("{combined_text}", combined_text)

                client = anthropic.Anthropic(api_key=api_key, base_url=base_url)
                message = client.messages.create(
                    model=model,
                    max_tokens=1024,
                    system=system_prompt,
                    messages=[{"role": "user", "content": current_user_prompt + str(text)}])
                answer = message.content[0]['text']
                if srt == "åŸå§‹è¯­è¨€ä¸ºé¦–":
                    srt_content[segment_id]['text'] = str(text) + "\n" + str(answer)
                elif srt == "ç›®æ ‡è¯­è¨€ä¸ºé¦–":
                    srt_content[segment_id]['text'] = str(answer) + "\n" + str(text)
                else:
                    srt_content[segment_id]['text'] = answer
                segment_id += 1
                print(answer)

                previous_texts.append(text)
                if len(previous_texts) > context_size:
                    previous_texts.pop(0)

                time.sleep(wait_time)

        elif "gemini" in model:
            print(f"\033[1;34mğŸŒ ä»£ç†åœ°å€: {base_url}\033[0m")
            if base_url is None:
                print("- Python SDK è°ƒç”¨ï¼ˆè‹¥æƒ³ä½¿ç”¨ä»£ç†ï¼Œè¯·å¡«å…¥å¯¹åº”çš„BASE_URLåè‡ªåŠ¨ä½¿ç”¨ä»£ç†æ¨¡å¼ï¼)")
                print(f"\033[1;34mğŸ“ ç¿»è¯‘å†…å®¹: \033[0m")
                segment_id = 0

                context_size = 2
                previous_texts = []
                original_user_prompt = user_prompt

                for segment in srt_content:
                    text = segment['text']

                    if len(previous_texts) > 0:
                        combined_text = "\n".join(previous_texts[-context_size:]) + "\n"
                    else:
                        combined_text = text
                    current_user_prompt = original_user_prompt.replace("{combined_text}", combined_text)

                    genai.configure(api_key=api_key)
                    model = genai.GenerativeModel(model)
                    answer = model.generate_content(system_prompt + current_user_prompt  + str(text))

                    if srt == "åŸå§‹è¯­è¨€ä¸ºé¦–":
                        srt_content[segment_id]['text'] = str(text) + "\n" + str(answer)
                    elif srt == "ç›®æ ‡è¯­è¨€ä¸ºé¦–":
                        srt_content[segment_id]['text'] = str(answer) + "\n" + str(text)
                    else:
                        srt_content[segment_id]['text'] = answer
                    segment_id += 1
                    print(answer.text)

                    previous_texts.append(text)
                    if len(previous_texts) > context_size:
                        previous_texts.pop(0)

                    time.sleep(wait_time)
            else:
                print("- Request è¯·æ±‚è°ƒç”¨ï¼ˆé€‚ç”¨äºä»£ç†æ¨¡å¼ï¼Œè‹¥ä¸æƒ³ä½¿ç”¨Requestæ¨¡å¼ï¼Œè¯·æŠŠBASE_URLç•™ç©ºï¼)")
                print(f"\033[1;34mğŸ“ ç¿»è¯‘å†…å®¹: \033[0m")
                segment_id = 0

                context_size = 2
                previous_texts = []
                original_user_prompt = user_prompt

                for segment in srt_content:
                    text = segment['text']

                    if len(previous_texts) > 0:
                        combined_text = "\n".join(previous_texts[-context_size:]) + "\n"
                    else:
                        combined_text = text
                    current_user_prompt = original_user_prompt.replace("{combined_text}", combined_text)

                    payload = json.dumps({
                        "model": model,
                        "messages": [
                            {
                                "role": "system",
                                "content": system_prompt
                            },
                            {
                                "role": "user",
                                "content": current_user_prompt + str(text)
                            }
                        ],
                        "temperature": 0.8
                    })
                    headers = {
                        'Content-Type': 'application/json',
                        'Authorization': api_key
                    }
                    response = requests.request("POST", base_url, headers=headers, data=payload)
                    answer = response.json()
                    try:
                        answer = answer["choices"][0]["message"]["content"]
                        if srt == "åŸå§‹è¯­è¨€ä¸ºé¦–":
                            srt_content[segment_id]['text'] = str(text) + "\n" + str(answer)
                        elif srt == "ç›®æ ‡è¯­è¨€ä¸ºé¦–":
                            srt_content[segment_id]['text'] = str(answer) + "\n" + str(text)
                        else:
                            srt_content[segment_id]['text'] = answer
                        segment_id += 1
                        print(answer.text)

                        previous_texts.append(text)
                        if len(previous_texts) > context_size:
                            previous_texts.pop(0)

                        time.sleep(wait_time)
                    except Exception as e:
                        print("-------è¯·é˜…è¯»ä¸‹æ–¹æŠ¥é”™å†…å®¹------\n")
                        print(f"An error occurred: {type(e).__name__}, with message: \n {answer}")
                        print("\n-------è¯·æ ¹æ®æŠ¥é”™æ£€æŸ¥ä½ çš„keyæˆ–è€…ä»£ç†-------\n")
                        raise e

        else:
            print(f"\033[1;34mğŸŒ ä»£ç†åœ°å€: {base_url}\033[0m")
            print(f"\033[1;34mğŸ“ ç¿»è¯‘å†…å®¹: \033[0m")
            client = OpenAI(api_key=api_key, base_url=base_url)

            segment_id = 0
            context_size = 2
            previous_texts = []
            original_user_prompt = user_prompt

            for segment in srt_content:
                text = segment['text']

                if len(previous_texts) > 0:
                    combined_text = "\n".join(previous_texts[-context_size:]) + "\n"
                else:
                    combined_text = text
                current_user_prompt = original_user_prompt.replace("{combined_text}", combined_text)

                response = client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": current_user_prompt + str(text)}
                    ])
                answer = response.choices[0].message.content
                if srt == "åŸå§‹è¯­è¨€ä¸ºé¦–":
                    srt_content[segment_id]['text'] = str(text) + "\n" + str(answer)
                elif srt == "ç›®æ ‡è¯­è¨€ä¸ºé¦–":
                    srt_content[segment_id]['text'] = str(answer) + "\n" + str(text)
                else:
                    srt_content[segment_id]['text'] = answer
                segment_id += 1
                print(answer)

                previous_texts.append(text)
                if len(previous_texts) > context_size:
                    previous_texts.pop(0)

                time.sleep(wait_time)
    return srt_content


def read_srt_file(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        srt = file.read().split('\n\n')
        timestamp_pattern = re.compile(r"(\d{2}:\d{2}:\d{2},\d{3} --> \d{2}:\d{2}:\d{2},\d{3})")
        subtitles = []

        for block in srt:
            lines = block.split('\n')
            if len(lines) >= 3:
                num = lines[0]
                times = lines[1]
                text = "\n".join(lines[2:])

                if timestamp_pattern.match(times):
                    subtitles.append({
                        'number': num,
                        'time': times,
                        'text': text
                    })
    return subtitles


def extract_frames(video_path, output_dir, time_interval=1):
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    video = cv2.VideoCapture(video_path)
    if not video.isOpened():
        print(f"\033[1;31mâŒ æ— æ³•è¯»å–è§†é¢‘å…³é”®å¸§ï¼Œè¯·æ£€æŸ¥æ­¤ç›®å½•æ˜¯å¦å­˜åœ¨:\033[0m\033[1;34m {video_path} \033[0m")
        st.toast("æ— æ³•è¯»å–è§†é¢‘å…³é”®å¸§", icon=":material/release_alert:")
        st.stop()

    timestamp = 0
    while True:
        ret, frame = video.read()
        if not ret:
            break

        current_timestamp = video.get(cv2.CAP_PROP_POS_MSEC) / 1000
        if current_timestamp >= timestamp + time_interval:
            timestamp = current_timestamp
            frame_filename = os.path.join(output_dir, f'frame_{int(timestamp)}.png')
            cv2.imwrite(frame_filename, frame)
    video.release()


def write_llms(view, language, key, url, model, text, token, temp):
    print(f"\033[1;34mğŸˆ´ å†™ä½œæ¨¡å‹: {model}\033[0m")
    global_key, global_url, all_in_one = get_global_info()
    if all_in_one:

        client = OpenAI(api_key=global_key, base_url=global_url)
        print(f"\033[1;34mğŸŒ ä»£ç†åœ°å€: {url}\033[0m")
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†æ¸Šåšä¸”æ“…é•¿åˆ†æçš„å†™ä½œåŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯å¸®åŠ©ç”¨æˆ·åŸºäºæ–‡æœ¬å†…å®¹é‡æ–°åˆ›ä½œä¸€ç¯‡æ–‡ç« æˆ–æ€»ç»“ï¼Œéœ€éµå¾ªä»¥ä¸‹è¦æ±‚ï¼š1. ç¡®ä¿æ–‡ç« ç»“æ„åˆç† 2. æ ¹æ®ç”¨æˆ·çš„è¦æ±‚è°ƒæ•´æ–‡ç« çš„è¯­æ°”ã€é£æ ¼ã€è§†è§’ 3.ç¡®ä¿æ–‡ç« ç»“æ„åˆç†ï¼Œé€»è¾‘æ¸…æ™°"},
                {"role": "user", "content": f"è¯·ä½ æ ¹æ®ä¸‹é¢çš„æ–‡æœ¬å†…å®¹å†™ä¸€ç¯‡{language}çš„æ–‡ç« æˆ–æ€»ç»“ï¼Œå†™ä½œæ—¶è¯·éµå¾ªä»¥ä¸‹è§„åˆ™ï¼š1. è¯·ä»¥{view}çš„è§†è§’å†™ä½œ 2. æ€»ç¯‡å¹…ä¸å¾—å°‘äº{token}å­— 3.ç¡®ä¿é€»è¾‘æ¸…æ™°ã€æ ¸å¿ƒå†…å®¹å®Œæ•´ã€‚æ–‡æœ¬å†…å®¹ï¼š\n{text}"}
            ])
        answer = response.choices[0].message.content
        print(f"\033[1;34mğŸ“ å†™ä½œå†…å®¹: \033[0m")
        print(answer)

    else:
        if "gpt" in model:
            print(f"\033[1;34mğŸŒ ä»£ç†åœ°å€: {url}\033[0m")
            print(f"\033[1;34mğŸ“ å†™ä½œå†…å®¹: \033[0m")
            client = OpenAI(api_key=key, base_url=url)
            response = client.chat.completions.create(
                model=model,
                temperature=temp,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†æ¸Šåšä¸”æ“…é•¿åˆ†æçš„å†™ä½œåŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯å¸®åŠ©ç”¨æˆ·åŸºäºæ–‡æœ¬å†…å®¹é‡æ–°åˆ›ä½œä¸€ç¯‡æ–‡ç« æˆ–æ€»ç»“ï¼Œéœ€éµå¾ªä»¥ä¸‹è¦æ±‚ï¼š1. ç¡®ä¿æ–‡ç« ç»“æ„åˆç† 2. æ ¹æ®ç”¨æˆ·çš„è¦æ±‚è°ƒæ•´æ–‡ç« çš„è¯­æ°”ã€é£æ ¼ã€è§†è§’ 3.ç¡®ä¿æ–‡ç« ç»“æ„åˆç†ï¼Œé€»è¾‘æ¸…æ™°"},
                    {"role": "user", "content": f"è¯·ä½ æ ¹æ®ä¸‹é¢çš„æ–‡æœ¬å†…å†™ä¸€ç¯‡{language}çš„æ–‡ç« æˆ–æ€»ç»“ï¼Œå†™ä½œæ—¶è¯·éµå¾ªä»¥ä¸‹è§„åˆ™ï¼š1. è¯·ä»¥{view}çš„è§†è§’å†™ä½œ 2. æ€»ç¯‡å¹…ä¸å¾—å°‘äº{token}å­— 3.ç¡®ä¿é€»è¾‘æ¸…æ™°ã€æ ¸å¿ƒå†…å®¹å®Œæ•´ã€‚æ–‡æœ¬å†…å®¹ï¼š\n{text}"}
                ]
            )
            answer = response.choices[0].message.content
            print(answer)

        elif "claude" in model:
            print(f"\033[1;34mğŸŒ ä»£ç†åœ°å€: {url}\033[0m")
            print(f"\033[1;34mğŸ“ å†™ä½œå†…å®¹: \033[0m")
            client = anthropic.Anthropic(api_key=key, base_url=url)
            message = client.messages.create(
                model=model,
                max_tokens=1024,
                temperature=temp,
                system="ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†æ¸Šåšä¸”æ“…é•¿åˆ†æçš„å†™ä½œåŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯å¸®åŠ©ç”¨æˆ·åŸºäºæ–‡æœ¬å†…å®¹é‡æ–°åˆ›ä½œä¸€ç¯‡æ–‡ç« æˆ–æ€»ç»“ï¼Œéœ€éµå¾ªä»¥ä¸‹è¦æ±‚ï¼š1. ç¡®ä¿æ–‡ç« ç»“æ„åˆç† 2. æ ¹æ®ç”¨æˆ·çš„è¦æ±‚è°ƒæ•´æ–‡ç« çš„è¯­æ°”ã€é£æ ¼ã€è§†è§’ 3.ç¡®ä¿æ–‡ç« ç»“æ„åˆç†ï¼Œé€»è¾‘æ¸…æ™°",
                messages=[{"role": "user", "content": f"è¯·ä½ æ ¹æ®ä¸‹é¢çš„æ–‡æœ¬å†…å®¹å†™ä¸€ç¯‡{language}çš„æ–‡ç« æˆ–æ€»ç»“ï¼Œå†™ä½œæ—¶è¯·éµå¾ªä»¥ä¸‹è§„åˆ™ï¼š1. è¯·ä»¥{view}çš„è§†è§’å†™ä½œ 2. æ€»ç¯‡å¹…ä¸å¾—å°‘äº{token}å­— 3.ç¡®ä¿é€»è¾‘æ¸…æ™°ã€æ ¸å¿ƒå†…å®¹å®Œæ•´ã€‚æ–‡æœ¬å†…å®¹ï¼š\n{text}"}]
            )

            answer = message.content[0]['text']

        elif "gemini" in model:
            if url is None:
                print("- Python SDK è°ƒç”¨ï¼šå¦‚æœéœ€è¦ä½¿ç”¨ä»£ç†ï¼Œè¯·å¡«å†™ BASE_URLï¼Œç³»ç»Ÿå°†è‡ªåŠ¨å¯ç”¨ä»£ç†æ¨¡å¼ï¼")
                print(f"\033[1;34mğŸŒ ä»£ç†åœ°å€: {url}\033[0m")
                genai.configure(client_options={"api_key": key})
                model = genai.GenerativeModel(model)
                answer = model.generate_content("è¯·ä½ æ ¹æ®ä¸‹é¢çš„æ–‡æœ¬å†…å®¹å†™ä¸€ç¯‡{language}çš„æ–‡ç« æˆ–æ€»ç»“ï¼Œå†™ä½œæ—¶è¯·éµå¾ªä»¥ä¸‹è§„åˆ™ï¼š1. è¯·ä»¥{view}çš„è§†è§’å†™ä½œ 2. æ€»ç¯‡å¹…ä¸å¾—å°‘äº{token}å­— 3.ç¡®ä¿é€»è¾‘æ¸…æ™°ã€æ ¸å¿ƒå†…å®¹å®Œæ•´ã€‚æ–‡æœ¬å†…å®¹ï¼š\n{text}")
                print(f"\033[1;34mğŸ“ å†™ä½œå†…å®¹: \033[0m")
                print(answer)
            else:
                print("- Request è¯·æ±‚è°ƒç”¨ï¼ˆé€‚ç”¨äºä»£ç†æ¨¡å¼ï¼Œè‹¥ä¸æƒ³ä½¿ç”¨Requestæ¨¡å¼ï¼Œè¯·æŠŠBASE_URLç•™ç©ºï¼)")
                print(f"\033[1;34mğŸŒ ä»£ç†åœ°å€: {url}\033[0m")
                print(f"\033[1;34mğŸ“ å†™ä½œå†…å®¹: \033[0m")

                payload = json.dumps({
                    "model": model,
                    "messages": [
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†æ¸Šåšä¸”æ“…é•¿åˆ†æçš„å†™ä½œåŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯å¸®åŠ©ç”¨æˆ·åŸºäºæ–‡æœ¬å†…å®¹é‡æ–°åˆ›ä½œä¸€ç¯‡æ–‡ç« æˆ–æ€»ç»“ï¼Œéœ€éµå¾ªä»¥ä¸‹è¦æ±‚ï¼š1. ç¡®ä¿æ–‡ç« ç»“æ„åˆç† 2. æ ¹æ®ç”¨æˆ·çš„è¦æ±‚è°ƒæ•´æ–‡ç« çš„è¯­æ°”ã€é£æ ¼ã€è§†è§’ 3.ç¡®ä¿æ–‡ç« ç»“æ„åˆç†ï¼Œé€»è¾‘æ¸…æ™°"},
                        {"role": "user", "content": f"è¯·ä½ æ ¹æ®ä¸‹é¢çš„æ–‡æœ¬å†…å®¹å†™ä¸€ç¯‡{language}çš„æ–‡ç« æˆ–æ€»ç»“ï¼Œå†™ä½œæ—¶è¯·éµå¾ªä»¥ä¸‹è§„åˆ™ï¼š1. è¯·ä»¥{view}çš„è§†è§’å†™ä½œ 2. æ€»ç¯‡å¹…ä¸å¾—å°‘äº{token}å­— 3.ç¡®ä¿é€»è¾‘æ¸…æ™°ã€æ ¸å¿ƒå†…å®¹å®Œæ•´ã€‚æ–‡æœ¬å†…å®¹ï¼š\n{text}"}
                    ],
                    "temperature": 0.8
                })
                headers = {'Content-Type': 'application/json', 'Authorization': key}
                response = requests.request("POST", url, headers=headers, data=payload)
                answer = response.json()

                try:
                    answer = answer["choices"][0]["message"]["content"]
                    print(answer)
                except Exception as e:
                    print("\n\033[1;31m------- è¯·é˜…è¯»ä¸‹æ–¹æŠ¥é”™å†…å®¹ -------\033[0m\n")
                    print(f"\033[1;31mâŒ å‘ç”Ÿé”™è¯¯: {type(e).__name__}\033[0m")
                    print(f"\033[1;33mğŸ’¬ é”™è¯¯è¯¦æƒ…: \033[0m\n{answer}\n")
                    print("\033[1;31m------- è¯·æ ¹æ®æŠ¥é”™å†…å®¹æ£€æŸ¥ä½ çš„ API Key æˆ–ä»£ç†é…ç½® -------\033[0m\n")
                    raise e

        else:
            client = OpenAI(api_key=key, base_url=url)
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†æ¸Šåšä¸”æ“…é•¿åˆ†æçš„å†™ä½œåŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯å¸®åŠ©ç”¨æˆ·åŸºäºæ–‡æœ¬å†…å®¹é‡æ–°åˆ›ä½œä¸€ç¯‡æ–‡ç« æˆ–æ€»ç»“ï¼Œéœ€éµå¾ªä»¥ä¸‹è¦æ±‚ï¼š1. ç¡®ä¿æ–‡ç« ç»“æ„åˆç† 2. æ ¹æ®ç”¨æˆ·çš„è¦æ±‚è°ƒæ•´æ–‡ç« çš„è¯­æ°”ã€é£æ ¼ã€è§†è§’ 3.ç¡®ä¿æ–‡ç« ç»“æ„åˆç†ï¼Œé€»è¾‘æ¸…æ™°"},
                    {"role": "user", "content": f"è¯·ä½ æ ¹æ®ä¸‹é¢çš„æ–‡æœ¬å†…å®¹å†™ä¸€ç¯‡{language}çš„æ–‡ç« æˆ–æ€»ç»“ï¼Œå†™ä½œæ—¶è¯·éµå¾ªä»¥ä¸‹è§„åˆ™ï¼š1. è¯·ä»¥{view}çš„è§†è§’å†™ä½œ 2. æ€»ç¯‡å¹…ä¸å¾—å°‘äº{token}å­— 3.ç¡®ä¿é€»è¾‘æ¸…æ™°ã€æ ¸å¿ƒå†…å®¹å®Œæ•´ã€‚æ–‡æœ¬å†…å®¹ï¼š\n{text}"}
                ])
            answer = response.choices[0].message.content
            print(f"\033[1;34mğŸ“ å†™ä½œå†…å®¹: \033[0m")
            print(answer)

    return answer


def encode_image(path):
    with open(path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')
