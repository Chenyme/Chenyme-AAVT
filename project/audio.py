import os
import toml
import time
import datetime
import streamlit as st
import streamlit_antd_components as sac
from openai import OpenAI
from .utils.utils2 import (openai_whisper_result, faster_whisper_result, file_to_mp3)


def audio():
    project_dir = os.path.dirname(os.path.abspath(__file__)).replace("\\", "/")
    config_dir = project_dir + "/config/"  # é…ç½®æ–‡ä»¶
    cache_dir = project_dir + "/cache/"  # æœ¬åœ°ç¼“å­˜

    api_config = toml.load(config_dir + "api.toml")  # åŠ è½½APIé…ç½®
    openai_key = api_config["GPT"]["openai_key"]
    openai_base = api_config["GPT"]["openai_base"]
    kimi_key = api_config["KIMI"]["kimi_key"]
    kimi_base = api_config["KIMI"]["kimi_base"]
    deepseek_key = api_config["DEEPSEEK"]["deepseek_key"]
    deepseek_base = api_config["DEEPSEEK"]["deepseek_base"]
    chatglm_key = api_config["CHATGLM"]["chatglm_key"]
    chatglm_base = api_config["CHATGLM"]["chatglm_base"]
    local_key = api_config["LOCAL"]["api_key"]
    local_base = api_config["LOCAL"]["base_url"]
    local_model = api_config["LOCAL"]["model_name"]

    video_config = toml.load(config_dir + "video.toml")  # åŠ è½½videoé…ç½®
    openai_whisper_api = video_config["WHISPER"]["openai_whisper_api"]  # openai_whisperé…ç½®
    faster_whisper_model = video_config["WHISPER"]["faster_whisper_model_default"]  # faster_whisperé…ç½®
    faster_whisper_local = video_config["WHISPER"]["faster_whisper_model_local"]  # æœ¬åœ°æ¨¡å‹åŠ è½½
    faster_whisper_local_path = video_config["WHISPER"]["faster_whisper_model_local_path"]  # æœ¬åœ°æ¨¡å‹è·¯å¾„
    gpu_setting = video_config["WHISPER"]["gpu"]
    vad_setting = video_config["WHISPER"]["vad"]
    lang_setting = video_config["WHISPER"]["lang"]
    min_vad_setting = video_config["MORE"]["min_vad"]
    beam_size_setting = video_config["MORE"]["beam_size"]
    whisper_prompt_setting = video_config["MORE"]["whisper_prompt"]
    temperature_setting = video_config["MORE"]["temperature"]

    st.title("AI å†…å®¹é—®ç­”åŠ©æ‰‹")
    with st.sidebar:
        uploaded_file = st.file_uploader("è¯·åœ¨è¿™é‡Œä¸Šä¼ è§†é¢‘ï¼š", type=["mp3", "mpga", "m4a", "wav", 'mp4', 'mov', 'avi', 'm4v', 'webm', 'flv', 'ico'], label_visibility="collapsed")

    col1, col2 = st.columns([0.3, 0.7])
    with col1:
        sac.divider("æ¨¡å‹", align='center', color='gray')
        if st.button("å¼€å§‹è¯†åˆ«", type="primary", use_container_width=True):
            if uploaded_file is not None:
                st.session_state.video_name = uploaded_file.name
                time1 = time.time()
                msg = st.toast('å¼€å§‹ç”Ÿæˆ!')
                msg.toast('æ­£åœ¨è¿›è¡Œè§†é¢‘æå–ğŸ“½ï¸')

                current_time = datetime.datetime.now().strftime("%Y-%m-%d %H-%M-%S")
                output_file = cache_dir + current_time
                os.makedirs(output_file)
                with open(output_file + '/' + uploaded_file.name, "wb") as file:
                    file.write(uploaded_file.getbuffer())
                print(f"- æœ¬æ¬¡ä»»åŠ¡ç›®å½•ï¼š{output_file}")
                if uploaded_file.name.split(".")[-1] != "mp3":
                    file_to_mp3(uploaded_file.name, output_file)

                time2 = time.time()
                msg.toast('æ­£åœ¨è¯†åˆ«è§†é¢‘å†…å®¹ğŸ”')
                if openai_whisper_api:
                    result = openai_whisper_result(openai_key, openai_base, output_file, whisper_prompt_setting, temperature_setting)
                else:
                    device = 'cuda' if gpu_setting else 'cpu'
                    model = faster_whisper_model
                    if faster_whisper_local:
                        model = faster_whisper_local_path
                    result = faster_whisper_result(output_file, device, "tiny", whisper_prompt_setting, temperature_setting, vad_setting, lang_setting, beam_size_setting, min_vad_setting)
                st.session_state.text = result["text"]
            else:
                st.toast("è¯·å…ˆä¸Šä¼ æ–‡ä»¶ï¼")

        with st.expander("æ¨¡å‹é€‰æ‹©", expanded=True):
            translate_option = sac.menu([
                sac.MenuItem('ChatGPT-OpenAI', icon='1-square-fill', children=[
                    sac.MenuItem('gpt-3.5-turbo', icon='robot'),
                    sac.MenuItem('gpt-4-turbo', icon='robot'),
                    sac.MenuItem('gpt-4o', icon='robot'),
                    sac.MenuItem('gpt-4v', icon='robot'),
                ]),
                sac.MenuItem('Kimi-æœˆä¹‹æš—é¢', icon='2-square-fill', children=[
                    sac.MenuItem('moonshot-v1-8k', icon='robot'),
                    sac.MenuItem('moonshot-v1-32k', icon='robot'),
                    sac.MenuItem('moonshot-v1-128k', icon='robot'),
                ]),
                sac.MenuItem('ChatGLM-æ™ºè°±AI', icon='3-square-fill', children=[
                    sac.MenuItem('glm-3-turbo', icon='robot'),
                    sac.MenuItem('glm-4v', icon='robot'),
                    sac.MenuItem('glm-4', icon='robot'),
                ]),
                sac.MenuItem('DeepSeek-æ·±åº¦æ±‚ç´¢', icon='4-square-fill', children=[
                    sac.MenuItem('deepseek-chat', icon='robot'),
                ]),
                sac.MenuItem('Local LLMs', icon='5-square-fill', children=[
                    sac.MenuItem('Local', icon='robot'),
                ]),
            ], variant='filled', indent=30, open_all=True, index=1)

    with col2:
        sac.divider("åŠ©æ‰‹", align='center', color='gray')
        with st.popover("**è®¾ç½®**", use_container_width=True):
            height = st.number_input("å¯¹è¯æ¡†é«˜åº¦", min_value=300, step=100, value=580)
            pre_prompt = st.text_input("Prompt", value="ä½ æ˜¯åŸºäºä»¥ä¸‹å†…å®¹çš„BOT,è¯·ç»“åˆè‡ªèº«çŸ¥è¯†å’Œå†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œå†…å®¹ï¼š\n")
            temperature_setting = st.number_input("Temperature", min_value=0.0, max_value=1., step=0.1, value=0.80)
        try:
            able = False if st.session_state.text else True
        except:
            able = True

        messages = st.container(height=height)
        if "messages1" not in st.session_state:
            st.session_state["messages1"] = [{"role": "assistant", "content": "æ‚¨å¯¹ä¸Šä¼ çš„å†…å®¹æœ‰ä»€ä¹ˆç–‘é—®?"}]

        for msg1 in st.session_state.messages1:
            messages.chat_message(msg1["role"]).write(msg1["content"])

        if prompt := st.chat_input(disabled=able, placeholder="åŸºäºä¸Šä¼ æ–‡ä»¶çš„çš„Chatï¼Œæ‚¨å¯ä»¥é—®ä»»ä½•å…³äºä¸Šä¼ æ–‡ä»¶çš„é—®é¢˜"):
            st.session_state.messages1.append({"role": "user", "content": prompt})
            messages.chat_message("user").write(prompt)

            if "moonshot" in translate_option:
                client = OpenAI(api_key=kimi_key, base_url=kimi_base)
            elif "glm" in translate_option:
                client = OpenAI(api_key=chatglm_key, base_url=chatglm_base)
            elif "deepseek" in translate_option:
                client = OpenAI(api_key=deepseek_key, base_url=deepseek_base)
            elif "local" in translate_option:
                translate_option = local_model
                client = OpenAI(api_key=local_key, base_url=local_base)

            response = client.chat.completions.create(model=translate_option,
                                                      messages=[{"role": "system", "content": pre_prompt + st.session_state.text},
                                                                {"role": "user", "content": prompt}],
                                                      temperature=temperature_setting)
            msg1 = response.choices[0].message.content
            st.session_state.messages1.append({"role": "assistant", "content": msg1})
            messages.chat_message("assistant").write(msg1)
