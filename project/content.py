import os
import toml
import time
import torch
import datetime
import streamlit as st
import streamlit_antd_components as sac
from openai import OpenAI
from .utils.utils2 import (openai_whisper_result, faster_whisper_result, file_to_mp3, check_ffmpeg, check_cuda_support)


def content():
    project_dir = os.path.dirname(os.path.abspath(__file__)).replace("\\", "/")
    config_dir = project_dir + "/config/"  # é…ç½®æ–‡ä»¶
    cache_dir = project_dir + "/cache/"  # æœ¬åœ°ç¼“å­˜
    model_dir = project_dir.replace("project", "model")

    api_config = toml.load(config_dir + "api.toml")  # åŠ è½½APIé…ç½®
    openai_key = api_config["GPT"]["openai_key"]
    openai_base = api_config["GPT"]["openai_base"]
    kimi_key = api_config["KIMI"]["kimi_key"]
    kimi_base = api_config["KIMI"]["kimi_base"]
    deepseek_key = api_config["DEEPSEEK"]["deepseek_key"]
    deepseek_base = api_config["DEEPSEEK"]["deepseek_base"]
    chatglm_key = api_config["CHATGLM"]["chatglm_key"]
    chatglm_base = api_config["CHATGLM"]["chatglm_base"]
    local_key = api_config["LOCAL"]["api_key"]
    local_base = api_config["LOCAL"]["base_url"]
    local_model = api_config["LOCAL"]["model_name"]

    content_config = toml.load(config_dir + "content.toml")  # åŠ è½½videoé…ç½®
    openai_whisper_api = content_config["WHISPER"]["openai_whisper_api"]  # openai_whisperé…ç½®
    faster_whisper_model = content_config["WHISPER"]["faster_whisper_model_default"]  # faster_whisperé…ç½®
    faster_whisper_local = content_config["WHISPER"]["faster_whisper_model_local"]  # æœ¬åœ°æ¨¡å‹åŠ è½½
    faster_whisper_local_path = content_config["WHISPER"]["faster_whisper_model_local_path"]  # æœ¬åœ°æ¨¡å‹è·¯å¾„
    gpu_setting = content_config["WHISPER"]["gpu"]
    beam_size_setting = content_config["MORE"]["beam_size"]
    temperature_setting = content_config["MORE"]["temperature"]
    log_setting = content_config["MORE"]["log"]

    options = {'faster-whisper': {'models': {'tiny': 0, 'tiny.en': 1, 'base': 2, 'base.en': 3, 'small': 4,
                                             'small.en': 5, 'medium': 6, 'medium.en': 7, 'large-v1': 8,
                                             'large-v2': 9, 'large-v3': 10, 'large': 11, 'distil-small.en': 12,
                                             'distil-medium.en': 13, 'distil-large-v2': 14,
                                             'distil-large-v3': 15}}}

    st.subheader("AI å†…å®¹é—®ç­”åŠ©æ‰‹")
    st.caption("AI Content Q&A Assistant")

    with st.sidebar:
        uploaded_file = st.file_uploader("è¯·åœ¨è¿™é‡Œä¸Šä¼ æ–‡ä»¶ï¼š", type=["mp3", "mpga", "m4a", "wav", 'mp4', 'mov', 'avi', 'm4v', 'webm', 'flv', 'ico'], label_visibility="collapsed")

    name = sac.segmented(
        items=[
            sac.SegmentedItem(label="å‚æ•°è®¾ç½®", icon="gear-wide-connected"),
            sac.SegmentedItem(label="å†…å®¹é—®ç­”", icon="file-check"),
            sac.SegmentedItem(label="GitHub", icon="github", href="https://github.com/Chenyme/Chenyme-AAVT"),
        ], align='center', size='sm', radius=20, color='red', divider=False, use_container_width=True
    )

    if name == "å‚æ•°è®¾ç½®":
        col1, col2 = st.columns([0.65, 0.35], gap="medium")
        with col1:
            with st.expander("**è¯†åˆ«è®¾ç½®**", expanded=True):
                model = st.selectbox("Whisperæ¨¡å¼", ("OpenAI-API æ¥å£è°ƒç”¨", "Faster-Whisper æœ¬åœ°éƒ¨ç½²"), index=0 if openai_whisper_api else 1, help="`OpenAI-API æ¥å£è°ƒç”¨`ï¼šä½¿ç”¨OpenAIçš„å®˜æ–¹æ¥å£è¿›è¡Œè¯†åˆ«ï¼Œæ–‡ä»¶é™åˆ¶25MBï¼ˆä¸æ˜¯ä¸Šä¼ è§†é¢‘æ–‡ä»¶ï¼Œæ˜¯è¯¥é¡¹ç›®è½¬æ¢åçš„éŸ³é¢‘æ–‡ä»¶ï¼Œå¯ä»¥å‰å¾€CacheæŸ¥çœ‹æ¯æ¬¡çš„å¤§å°ï¼‰ï¼Œè¿‡å¤§ä¼šå¯¼è‡´ä¸Šä¼ å¤±è´¥\n\n`Faster-Whisper æœ¬åœ°éƒ¨ç½²`ï¼šæœ¬åœ°è¯†åˆ«å­—å¹•ï¼Œæ— éœ€æ‹…å¿ƒå¤§å°é™åˆ¶ã€‚è¯·æ³¨æ„ï¼Œè‹¥ç½‘ç»œä¸ä½³è¯·å¯ç”¨ä¸‹æ–¹çš„æœ¬åœ°æ¨¡å‹åŠ è½½")
                if model == "OpenAI-API æ¥å£è°ƒç”¨":
                    openai_whisper_api = True
                    content_config["WHISPER"]["openai_whisper_api"] = openai_whisper_api
                    local_on = False
                    content_config["WHISPER"]["faster_whisper_model_local"] = local_on

                else:
                    col3, col4 = st.columns(2, gap="medium")
                    openai_whisper_api = False
                    content_config["WHISPER"]["openai_whisper_api"] = openai_whisper_api
                    with col3:
                        local_on = st.checkbox('æœ¬åœ°æ¨¡å‹', faster_whisper_local, help="ä½¿ç”¨æœ¬åœ°ä¸‹è½½å¥½çš„æ¨¡å‹è¿›è¡Œè½¬å½•")
                    content_config["WHISPER"]["faster_whisper_model_local"] = local_on

                if not openai_whisper_api:
                    with col4:
                        gpu = st.checkbox('GPUåŠ é€Ÿ', disabled=not torch.cuda.is_available(), help='cudaã€pytorchæ­£ç¡®åæ‰å¯ä½¿ç”¨ï¼', value=gpu_setting)
                        content_config["WHISPER"]["gpu"] = gpu

                    if local_on:
                        model_names = os.listdir(model_dir)
                        path = faster_whisper_local_path

                        try:
                            index_model = model_names.index(path.replace(model_dir + '/', ''))
                            local_option = st.selectbox('æœ¬åœ°æ¨¡å‹', model_names, index=index_model, help="æ¨¡å‹ä¸‹è½½ï¼šhttps://huggingface.co/Systran")
                        except:
                            local_option = st.selectbox('æœ¬åœ°æ¨¡å‹', model_names, index=0, help="æ¨¡å‹ä¸‹è½½ï¼šhttps://huggingface.co/Systran")
                        local_model_path = model_dir + '/' + local_option
                        content_config["WHISPER"]["faster_whisper_model_local_path"] = local_model_path

                    elif not local_on:
                        model_option = st.selectbox('è¯†åˆ«æ¨¡å‹', list(options['faster-whisper']['models'].keys()), index=options['faster-whisper']['models'][faster_whisper_model], help="æ¨èlargeæ¨¡å‹")
                        content_config["WHISPER"]["faster_whisper_model_default"] = model_option

            with st.expander("**é«˜çº§è®¾ç½®**", expanded=True):
                beam_size = st.number_input("æŸæœç´¢å¤§å°", min_value=1, max_value=20, value=beam_size_setting, step=1, disabled=openai_whisper_api, help="`beam_size`å‚æ•°ã€‚ç”¨äºå®šä¹‰æŸæœç´¢ç®—æ³•ä¸­æ¯ä¸ªæ—¶é—´æ­¥ä¿ç•™çš„å€™é€‰é¡¹æ•°é‡ã€‚æŸæœç´¢ç®—æ³•é€šè¿‡åœ¨æ¯ä¸ªæ—¶é—´æ­¥é€‰æ‹©æœ€æœ‰å¯èƒ½çš„å€™é€‰é¡¹æ¥æ„å»ºæœç´¢æ ‘ï¼Œå¹¶æ ¹æ®å€™é€‰é¡¹çš„å¾—åˆ†è¿›è¡Œæ’åºå’Œå‰ªæã€‚è¾ƒå¤§çš„beam_sizeå€¼ä¼šä¿ç•™æ›´å¤šçš„å€™é€‰é¡¹ï¼Œæ‰©å¤§æœç´¢ç©ºé—´ï¼Œå¯èƒ½æé«˜ç”Ÿæˆç»“æœçš„å‡†ç¡®æ€§ï¼Œä½†ä¹Ÿä¼šå¢åŠ è®¡ç®—å¼€é”€ã€‚ç›¸åï¼Œè¾ƒå°çš„beam_sizeå€¼ä¼šå‡å°‘è®¡ç®—å¼€é”€ï¼Œä½†å¯èƒ½å¯¼è‡´æœç´¢è¿‡æ—©åœ°æ”¾å¼ƒæœ€ä½³åºåˆ—ã€‚")
                temperature = st.number_input("Whisperæ¸©åº¦", min_value=0.0, max_value=1.0, value=temperature_setting, step=0.1, help="Whisperè½¬å½•æ—¶æ¨¡å‹æ¸©åº¦ï¼Œè¶Šå¤§éšæœºæ€§ï¼ˆåˆ›é€ æ€§ï¼‰è¶Šé«˜ã€‚")
                log = st.selectbox("FFmpeg-æ—¥å¿—çº§åˆ«", ["quiet", "panic", "fatal", "error", "warning", "info", "verbose", "debug", "trace"], index=["quiet", "panic", "fatal", "error", "warning", "info", "verbose", "debug", "trace"].index(log_setting), help="FFmpegè¾“å‡ºæ—¥å¿—ã€‚\n- **quiet**ï¼šæ²¡æœ‰è¾“å‡ºæ—¥å¿—ã€‚\n- **panic**ï¼šä»…åœ¨ä¸å¯æ¢å¤çš„è‡´å‘½é”™è¯¯å‘ç”Ÿæ—¶è¾“å‡ºæ—¥å¿—ã€‚\n- **fatal**ï¼šä»…åœ¨è‡´å‘½é”™è¯¯å‘ç”Ÿæ—¶è¾“å‡ºæ—¥å¿—ã€‚\n- **error**ï¼šåœ¨é”™è¯¯å‘ç”Ÿæ—¶è¾“å‡ºæ—¥å¿—ã€‚\n- **warning**ï¼šåœ¨è­¦å‘Šçº§åˆ«åŠä»¥ä¸Šçš„äº‹ä»¶å‘ç”Ÿæ—¶è¾“å‡ºæ—¥å¿—ã€‚\n- **info**ï¼šåœ¨ä¿¡æ¯çº§åˆ«åŠä»¥ä¸Šçš„äº‹ä»¶å‘ç”Ÿæ—¶è¾“å‡ºæ—¥å¿—ã€‚\n- **verbose**ï¼šè¾“å‡ºè¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬è°ƒè¯•å’Œä¿¡æ¯çº§åˆ«çš„æ—¥å¿—ã€‚\n- **debug**ï¼šè¾“å‡ºè°ƒè¯•ä¿¡æ¯ï¼Œéå¸¸è¯¦ç»†çš„æ—¥å¿—è¾“å‡ºã€‚\n- **trace**ï¼šæœ€è¯¦ç»†çš„æ—¥å¿—è¾“å‡ºï¼Œç”¨äºæå…¶è¯¦ç»†çš„è°ƒè¯•ã€‚")

                content_config["MORE"]["beam_size"] = beam_size
                content_config["MORE"]["temperature"] = temperature
                content_config["MORE"]["log"] = log

        with col2:
            if st.button("ä¿å­˜æ‰€æœ‰å‚æ•°", type="primary", use_container_width=True):
                sac.divider(label='**å‚æ•°æç¤º**', icon='activity', align='center', color='gray')
                with open(config_dir + '/content.toml', 'w', encoding='utf-8') as file:
                    toml.dump(content_config, file)
                sac.alert(
                    label='**å‚æ•°è®¾ç½® å·²ä¿å­˜**',
                    description='**æ‰€æœ‰å‚æ•°å…¨éƒ¨ä¿å­˜å®Œæ¯•**',
                    size='lg', radius=20, icon=True, closable=True, color='success')
            else:
                sac.divider(label='**å‚æ•°æç¤º**', icon='activity', align='center', color='gray')
                sac.alert(
                    label='**å‚æ•°è®¾ç½® å¯èƒ½æœªä¿å­˜**',
                    description='é‡æ–°è®¾ç½®åè¯·ç‚¹å‡»ä¿å­˜',
                    size='lg', radius=20, icon=True, closable=True, color='error')

            if check_ffmpeg():
                if check_cuda_support():
                    sac.alert(
                        label='**FFmpeg GPUåŠ é€Ÿæ­£å¸¸**',
                        description='FFmpeg**åŠ é€Ÿå¯ç”¨**',
                        size='lg', radius=20, icon=True, closable=True, color='success')
                else:
                    sac.alert(
                        label='**FFmpeg çŠ¶æ€æ­£å¸¸**',
                        description='å·²**æˆåŠŸæ£€æµ‹**åˆ°FFmpeg',
                        size='lg', radius=20, icon=True, closable=True, color='success')
            else:
                sac.alert(
                    label='**FFmpeg çŠ¶æ€é”™è¯¯**',
                    description='**æœªæ£€æµ‹åˆ°**FFmpeg',
                    size='lg', radius=20, icon=True, closable=True, color='success')

            if openai_whisper_api:
                sac.alert(
                    label='**Whipser APIè°ƒç”¨å·²å¼€å¯**',
                    description='ç¡®ä¿**OPENAIç›¸å…³é…ç½®ä¸ä¸ºç©º**',
                    size='lg', radius=20, icon=True, closable=True, color='warning')

            if not openai_whisper_api:
                if gpu:
                    sac.alert(
                        label='**GPUåŠ é€Ÿæ¨¡å¼ å·²å¼€å¯**',
                        description='**è‹¥æœªCUDA11è¯·å‚é˜…[AAVT](https://zwho5v3j233.feishu.cn/wiki/OGcrwinzhi88MkkvEMVcLkDgnzc?from=from_copylink)**',
                        size='lg', radius=20, icon=True, closable=True, color='warning')

            if not openai_whisper_api:
                if local_on:
                    sac.alert(
                        label='**Whisper æœ¬åœ°åŠ è½½å·²å¼€å¯**',
                        description='[æ¨¡å‹ä¸‹è½½](https://huggingface.co/Systran) | [ä½¿ç”¨æ–‡æ¡£](https://zwho5v3j233.feishu.cn/wiki/OGcrwinzhi88MkkvEMVcLkDgnzc?from=from_copylink)',
                        size='lg', radius=20, icon=True, closable=True, color='warning')

            if not torch.cuda.is_available():
                sac.alert(
                    label='**CUDA/Pytorch é”™è¯¯**',
                    description='è¯·æ£€æŸ¥ï¼**ä»…ä½¿ç”¨CPUè¯·å¿½ç•¥**',
                    size='lg', radius=20, icon=True, closable=True, color='error')

            sac.divider(label='POWERED BY @CHENYME', icon="lightning-charge", align='center', color='gray', key="1")

    if name == "å†…å®¹é—®ç­”":
        col1, col2 = st.columns([0.75, 0.25])
        with col2:
            if st.button("å¼€å§‹è¯†åˆ«", type="primary", use_container_width=True):
                if uploaded_file is not None:
                    st.session_state.video_name = "uploaded." + uploaded_file.name.split('.')[-1]
                    time1 = time.time()
                    msg = st.toast('å¼€å§‹ç”Ÿæˆ!')
                    msg.toast('æ­£åœ¨è¿›è¡Œè§†é¢‘æå–ğŸ“½ï¸')

                    current_time = datetime.datetime.now().strftime("%Y-%m-%d %H-%M-%S")
                    output_file = cache_dir + current_time
                    os.makedirs(output_file)
                    with open(output_file + '/' + st.session_state.video_name, "wb") as file:
                        file.write(uploaded_file.getbuffer())
                    print(f"- æœ¬æ¬¡ä»»åŠ¡ç›®å½•ï¼š{output_file}")
                    if uploaded_file.name.split(".")[-1] != "mp3":
                        file_to_mp3(log_setting, st.session_state.video_name, output_file)

                    time2 = time.time()
                    msg.toast('æ­£åœ¨è¯†åˆ«è§†é¢‘å†…å®¹ğŸ”')
                    if openai_whisper_api:
                        result = openai_whisper_result(openai_key, openai_base, output_file, "Donâ€™t make each line too long.", temperature_setting)
                    else:
                        device = 'cuda' if gpu_setting else 'cpu'
                        model = faster_whisper_model
                        if faster_whisper_local:
                            model = faster_whisper_local_path
                        result = faster_whisper_result(output_file, device, model, "Donâ€™t make each line too long.", temperature_setting, False, "è‡ªåŠ¨è¯†åˆ«", beam_size_setting, 500)
                    st.session_state.text = result["text"]
                    st.toast("å·²è¯†åˆ«å®Œæˆï¼Œå¼€å§‹å¯¹è¯å­ï¼", icon=":material/task_alt:")
                else:
                    st.toast("è¯·å…ˆä¸Šä¼ æ–‡ä»¶ï¼")

            with st.expander("æ¨¡å‹é€‰æ‹©", expanded=True):
                translate_option = sac.chip(
                    items=[
                        sac.ChipItem('gpt-3.5-turbo', icon='robot'),
                        sac.ChipItem('gpt-4-turbo', icon='robot'),
                        sac.ChipItem('gpt-4o', icon='robot'),
                        sac.ChipItem('gpt-4v', icon='robot'),
                        sac.ChipItem('moonshot-v1-8k', icon='robot'),
                        sac.ChipItem('moonshot-v1-32k', icon='robot'),
                        sac.ChipItem('moonshot-v1-128k', icon='robot'),
                        sac.ChipItem('glm-3-turbo', icon='robot'),
                        sac.ChipItem('glm-4v', icon='robot'),
                        sac.ChipItem('glm-4', icon='robot'),
                        sac.ChipItem('gpt-4o', icon='robot'),
                        sac.ChipItem('gpt-4v', icon='robot'),
                        sac.ChipItem('deepseek-chat', icon='robot'),
                        sac.ChipItem('Local', icon='robot'),
                    ], align='start', direction='vertical', radius='md', variant='filled', index=0
                )

        with col1:
            with st.popover("**å¯¹è¯è®¾ç½®**", use_container_width=True):
                height = st.number_input("å¯¹è¯æ¡†é«˜åº¦", min_value=300, step=100, value=590)
                pre_prompt = st.text_input("Prompt", value="ä½ æ˜¯åŸºäºä»¥ä¸‹å†…å®¹çš„BOT,è¯·ç»“åˆè‡ªèº«çŸ¥è¯†å’Œå†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œå†…å®¹ï¼š\n")
                temperature_setting = st.number_input("Temperature", min_value=0.0, max_value=1., step=0.1, value=0.80)
            try:
                able = False if st.session_state.text else True
            except:
                able = True

            messages = st.container(height=height)
            if "messages1" not in st.session_state:
                st.session_state["messages1"] = [{"role": "assistant", "content": "æ‚¨å¯¹ä¸Šä¼ çš„å†…å®¹æœ‰ä»€ä¹ˆç–‘é—®?"}]

            for msg1 in st.session_state.messages1:
                messages.chat_message(msg1["role"]).write(msg1["content"])

            if prompt := st.chat_input(disabled=able, placeholder="åŸºäºä¸Šä¼ æ–‡ä»¶çš„çš„Chatï¼Œæ‚¨å¯ä»¥é—®ä»»ä½•å…³äºä¸Šä¼ æ–‡ä»¶çš„é—®é¢˜"):
                st.session_state.messages1.append({"role": "user", "content": prompt})
                messages.chat_message("user").write(prompt)

                if "gpt" in translate_option:
                    client = OpenAI(api_key=openai_key, base_url=openai_base)
                elif "moonshot" in translate_option:
                    client = OpenAI(api_key=kimi_key, base_url=kimi_base)
                elif "glm" in translate_option:
                    client = OpenAI(api_key=chatglm_key, base_url=chatglm_base)
                elif "deepseek" in translate_option:
                    client = OpenAI(api_key=deepseek_key, base_url=deepseek_base)
                elif "local" in translate_option:
                    translate_option = local_model
                    client = OpenAI(api_key=local_key, base_url=local_base)

                response = client.chat.completions.create(model=translate_option,
                                                          messages=[{"role": "system", "content": pre_prompt + st.session_state.text},
                                                                    {"role": "user", "content": prompt}],
                                                          temperature=temperature_setting)
                msg1 = response.choices[0].message.content
                st.session_state.messages1.append({"role": "assistant", "content": msg1})
                messages.chat_message("assistant").write(msg1)

        sac.divider(label='POWERED BY @CHENYME', icon="lightning-charge", align='center', color='gray', key="4")
